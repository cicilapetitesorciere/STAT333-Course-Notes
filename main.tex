\documentclass{report}

\usepackage{tcolorbox}
\usepackage{mdframed}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{svg}
\usepackage{times}
\usepackage{tabularx}

\usepackage{nicefrac}



\begin{document}

\setlength\parindent{0pt}

\newcommand{\todo}{TODO\quad}

\newmdtheoremenv[
    backgroundcolor=green!5!white,
    linecolor=green!75!black,
    font=\normalfont\bfseries,
    innertopmargin=6pt,
    splittopskip=\topskip,
]{example}{Example}[section]

\newmdtheoremenv[
    backgroundcolor=blue!5!white,
    linecolor=blue!75!black,
    font=\normalfont\bfseries,
    innertopmargin=6pt,
    splittopskip=\topskip,
]{definition}{Definition}[section]

\newmdtheoremenv[
    backgroundcolor=yellow!5!white,
    linecolor=yellow!75!black,
    font=\normalfont\bfseries,
    innertopmargin=6pt,
    splittopskip=\topskip,
]{theorem}{Theorem}[section]

\newmdtheoremenv[
    backgroundcolor=yellow!5!white,
    linecolor=yellow!75!black,
    font=\normalfont\bfseries,
    innertopmargin=6pt,
    splittopskip=\topskip,
]{lemma}{Lemma}[section]

\newtcolorbox[auto counter,number within=section]{notsofast}[1][]{
    colback=red!5!white,
    colframe=red!75!black,
    fonttitle=\bfseries,
    title=\emph{Not So Fast!}
}


\tableofcontents

\chapter{Univariate Distributions and Generating Functions}

\section{Laws of Probability}
Imagine you have have a coin and you flip it twice in a row. One of several things may happen. Perhaps you get two heads ($HH$), or two tails ($TT$), or perhaps some combination of the two ($HT$ or $TH$). We can list all the possibilities of what might happen, \emph{however} we do not know what \emph{will} happen. We will observe many situations like this this course and in real life, and we refer to them as \textbf{Random Experiments}. The \textbf{Sample Space} of a Random Experiment (often denoted as $S$ or $\Omega$) is the set of all possible outcomes we may observe. For instance:
\[
    \Omega=\{HH,HT,TH,TT\}
\]

\begin{notsofast}
The Sample Space is not unique! We may define it differently depending on what properties we are interested in. For instance, suppose we do not care about the order of the coins but simply how many heads we observe. In this case we could define the Sample Space as
\[
    \Omega=\{0,1,2\}
\]
which is equally valid.
\end{notsofast}
We may also be interested in certain subsets of the Sample Space. For instance: what are the different ways we can get the same coin twice?
\[
    \{HH,TT\}\subseteq \Omega
\]
We refer to such subsets as \textbf{Events}. For the purposes of this course, any subset of the Sample Space can be considered an Event (\todo CONFIRM!). Any two Events $A$ and $B$ are \textbf{Mutually Exclusive} if $A\cup B=\emptyset$. From here we get the definition of \textbf{Probability}:
\begin{definition}[Probability]
A Probability is any function which maps Events to $\mathbb R$ and satisfies the following axioms:
\begin{enumerate}
    \item 
    \begin{enumerate}
        \item $P(\emptyset) = 0$
        \item $P(\Omega)=1$
    \end{enumerate}
    \item If $A_1, A_2,...$ are Mutually Exclusive Events, then
    \[
        P\left(\ \bigcup_{i=1}^\infty A_i\ \right) = \sum_{i=1}^\infty P(A_i)
    \]
\end{enumerate}
\end{definition}
\label{def:probability}
From this one small definition, we can derive the entire field of probability theory, including the following lemmas:

\begin{lemma}
    For any Event $A$ with compliment $\bar A$, $P(\bar A)=1-P(A)$
\begin{proof}
    Since $A$ and $\bar A$ are compliments of one another, they are mutually exclusive and satisfy
    \[
        A\cup \bar A = \Omega
    \]
    and hence
    \[
        P(A)+P(\bar A) = P(\Omega) = 1
    \]
    giving us
    \[
        P(\bar A)=1-P(A)
    \]
\end{proof}
\end{lemma}

\begin{lemma}
    For any two Events $A$ and $B$ such that $A\subseteq B$, $P(A)\le P(B)$.
    
    \begin{proof}
    Consider the Event $B\backslash A$, which is mutually exclusive to $A$. Since $A\subseteq B$ we may express $B$ as
    \[
        B=A\cup (B\backslash A)
    \]
    giving us
    \[
        P(B)=P(A)+P(B\backslash A)
    \]
    It can be shown that the probability of any Event is greater than or equal to zero (this is left as an exercise to the reader), and therefore $P(B)\le P(A)$.
    \end{proof}
\end{lemma}

One special case of Definition \ref{def:probability} is known as \textbf{The Classical Definition of Probability}. \todo CLASSICAL DEFINITION

\subsection*{End-of-the-section Problems}\label{sec:problaw_eosp}

\begin{enumerate}
    \item Which of the following has the highest probability?
    \begin{enumerate}
        \item Observing at least one six in 6 rolls of a fair die
        \item At least two sixes in 12 rolls of a fair die
        \item At least three sixes in 18 rolls of a fair die
    \end{enumerate}
    \item Suppose $n$ random strangers meet at a party. Assuming all birthdays are equally likely (and ignoring leap years, twins, etc.) show that if $n\ge 23$ then the probability of two people sharing the same birthday is greater than \nicefrac 12.
    \item Suppose two fair dice are rolled. What is the probability of getting at least one six?
    \item\textbf{(The Secretary's Problem)}
    Suppose you have applied to three jobs and are waiting to hear back. You are pretty sure that all three companies will give you an offer, but you do not know what salary they will offer you. And after each company presents their offer, you will have to decide whether to accept or reject it, and will not have time wait around to see what the next company is planning to offer you. What is the optimal strategy for maximizing your chances of accepting the best offer?
    \item \textbf{(DeMontfort's Problem)} 
    \begin{enumerate}
        \item Suppose you have 4 envelopes and 4 letters which you place in the envelopes at random such that each envelope contains exactly one letter. Find the probability that at least one of of the 4 letters ends up in the correct envelope.
        \item Find the general solution for $n$ letters and $n$ envelopes, and show what happens when $n\to\infty$.
    \end{enumerate}
    \item \textbf{(Simpson's Paradox)} Suppose we have two doctors (we will call them Dr. Bart and Dr. Lisa) each performing two operations (for instance: neurosurgery and bandaging a wound). Both doctors perform each surgery with some success rate. Now here's the riddle: is it possible that Dr. Lisa is better than Dr. Bart at both neurosurgery and bandaging, and yet have a lower overall success-rate? How might this strategy generalize to $n$ offers.
\end{enumerate}

\section{Conditional Probability}
Let $A$ and $B$ be two Events. Then the probability of $A$ given that $B$ has occurred, denoted b y $P(A\vert B)$ is given by
\[
    P(A\vert B)=\frac{P(A\cap B)}{P(B)}
\]
We often call $P(A)$ the \textbf{Prior Probability} and $P(A\vert B)$ the \textbf{Posterior Probability}.
\begin{notsofast}
    A common mistake is to assume that $P(A\vert B)=P(B\vert A)$, however this is not the case. This mistake is often known as ``the Prosecutor's Fallacy". We will discuss the high-profile case from where this name originated later in the section.
\end{notsofast}

\begin{figure}
    \centering
    \includesvg{figures/dependence.svg}
    \caption{An illustration of Conditional Probability. The Prior Probability $P(A)$ is $\nicefrac 5{10}$, however the Posterior Probability $P(A|B)$ is $\nicefrac 35$}
    \label{fig:dependence}
\end{figure}

\subsection{LOTP and Bayes Rule}
The definition of $P(A\vert B)$ is deceptively simple, but it has wide-ranging consequences. But first, we must establish yet more terminology: A \textbf{Partition} of $\Omega$ is any set of Mutually Exclusive Events $B_1,B_2,...,B_n$ such that $B_1\cup B_2\cup\cdots \cup B_n=\Omega$.

\begin{theorem}[Law of Total Probability] Let $B_1,B_2,...,B_n$ be some Partition of $\Omega$. Then for any Event $A$
\[
    P(A)=\sum_{i=1}^n P(A\vert B_i) P(B_i)
\]

\begin{proof}
    Since $B_1,B_2,...,B_n$ form a Partition of $\Omega$, we can decompose $A$ into the following union of Mutually Exclusive Events:
    \[
        A=(A\cap B_1)\cup (A\cap B_2) \cup \cdots \cup (A\cap B_n)
    \]
    \begin{center}
        \includesvg{figures/lotp.svg}
    \end{center}
    Then by Definition \ref{def:probability}
    \[
        P(A)=P(A\cap B_1)+ P(A\cap B_2)+ \cdots + P(A\cap B_n)
    \]
\end{proof}
\end{theorem}

\begin{example}
    Suppose you have two coins. One is fair, whereas the other one has $P(H)=\nicefrac 34$. One coin is picked at random and tossed 3 times. Find the probability that all three tosses return \emph{Heads}.

Let $F$ be the Event that a fair coin is chosen out of the two, and let $H^3$ be the probability of observing 3 heads. If we assume that there is an equal chance of choosing each coin, then the Prior Probability of observing $F$ and $\bar F$ are $P(F)=P(\bar F)=\nicefrac 12$, and the Posterior Probabilities of $H^3$ are
\[
    P(H^3 | F) = \left(\frac 12\right)^3;\quad P(H^3 | \bar F) = \left(\frac 34\right)^3
\]
Since $F$ and $\bar F$ are a Partition of the Sample Space, we may then use the Law of Total Probability to find
\[
    P(H^3) = P(H^3|F)P(F)+P(H^3|\bar F)P(\bar F)= \left(\frac 12\right)^3\times \frac 12 + \left(\frac 34\right)^3\times\frac 12
\]
or around $27\%$.
\end{example}
    

\begin{theorem}[Bayes Rule]
    Let $B_1,B_2,...,B_n$ be some Partition of $\Omega$. Then for any Event $A$ and any $i\in \{1,2,...,n\}$:
    \[
        P(B_i\vert A) = \frac{P(A\vert B_i)P(B_i)}{P(A)}
    \]
    or equivalently, using the Law of Total Probability
    \[
        P(B_i\vert A) = \frac{P(A\vert B_i)P(B_i)}{\sum_{i=j}^n P(A|B_j)P(B_j)}
    \]
    
    The proof of this is left as an exercise to the reader. 
\end{theorem}

\begin{example}
    Suppose you have two coins. One is fair, whereas the other one has $P(H)=\nicefrac 34$. You pick one coin at random and flip it three times. All three tosses turn up heads. What is the probability that the fair coin was chosen?

    We are trying to find $P(F|H^3)$. By Bayes Rule
    \[
        P(F | H^3) = \frac {P(H^3| F)P(F)}{P(H^3)} = \frac{(\nicefrac 12)^3\times \nicefrac 12}{\left(\nicefrac 12\right)^3\times \nicefrac 12 + \left(\nicefrac 34\right)^3\times\nicefrac 12}
    \]
    or around $23\%$.
\end{example}

\subsection{The Prosecutor's Fallacy}
Sally Clark was a British woman who lost both of her two children (apparently) to SIDS the late 1990s. This raised suspicion leading her to be tried for murder. The prosecution's case hinged on Sir Roy Meadows' argument that SIDS is very rare, only affecting around 1 in every 8500 children. Therefore the probability of two children dying of SIDS $(1/8500)^2$. In other words, there is a $99.9999986\%$ chance that she is guilty, which clearly goes beyond any reasonable doubt one could have, and so the jury convicted her (though this conviction was later overturned). There are two issues with Meadows' argument:
First of all, the it assumes that the two deaths are are Independent Events. But are they? Perhaps there is a genetic component to SIDS which both children had, or something in the shared environment. Hence the probability of two children in the same family dying may not be all that much higher than the probability of just one. This is an important point, however on its own it does little do dispel the evidence against Mrs. Clark, since even the probability of only one child dying is rather low. In fact, had the courts tried Mrs. Clark after her only her first child, they could have made a similar argument and claimed there was a $99.988\%$ chance she was guilty based on that alone. So then by this logic, should all cases of SIDS be treated not only with suspicion but as outright proof of fowl play? Clearly this would be preposterous, and it is here we begin to really see the cracks in the Meadows' argument.
\\\\
Let $E$ be the probability of having two children die of SIDS, and let $I$ be the probability of the mother being innocent. If we do assume that each child dying is an Independent Event, then it is true that $P(E|I)=1/8500^2$. However, Meadows claim conflated this with $P(I|E)$. These quantities are not necessarily equal, however they are related, so it is worth asking: what \emph{can} we learn about $P(I|E)$ from $P(E|I)$. Using Bayes Rule, 
\[
    P(I|E)=\frac{P(E|I)P(I)}{P(E)}=\frac 1 {72\,250\,000}\times \frac{P(I)}{P(E)}
\]

\todo finish????

\todo https://forensicstats.org/blog/2018/02/16/misuse-statistics-courtroom-sally-clark-case/

\section{Statistical Independence}
\subsection{Two Events}
Two Events $A$ and $B$ are \textbf{Independent} if they do not have any ``information" about the other. In other words knowing that $B$ has occurred does not change the probability of $A$ having occurred. We can express this symbolically as follows
\[
    P(A|B)=P(A)
\]
or equivalently:
\[
    P(A\cap B)=P(A)P(B)
\]
\begin{notsofast}
    Independence is \emph{not} the same as Mutual Exclusivity. In fact if $A$ and $B$ are Mutually Exclusive then they cannot be independent. 
    \begin{proof}
        Suppose $A$ and $B$ are two Mutually Exclusive Events and we know that $B$ has occurred. Then we also know $A$ must not have occurred. In other words, knowing $B$ has occurred has given us information about the probability of $A$ (namely that the probability is zero). 
    \end{proof}
\end{notsofast}

\begin{lemma}
Let $A$ and $B$ be Independent Events. Then the following pairs are also Independent:
\begin{enumerate}
    \item[i.] $A$ and $\bar B$
    \item[ii.] $\bar A$ and $B$
    \item[iii.] $\bar A$ and $\bar B$
\end{enumerate}

\begin{proof}
    \begin{enumerate}
        \item[]
        \item[i.] 
        \item[ii.] 
        \item[iii.] 
    \end{enumerate}
\end{proof}
\end{lemma}

\subsection{More than two Events}

\subsection{Conditional Independence}


Two events are \textbf{Conditionally Independent} given $C$ if 
\[
    P(A\cap B | C)= P(A|C)P(B|C)
\]

\todo






\section{Simpson's Paradox}
Let us return to one of the problems we considered at the end of \ref{sec:problaw_eosp}: Suppose we have two doctors (we will call them Bart and Lisa) each performing two operations (for instance: neurosurgery and bandaging a wound). Both doctors perform each surgery with some success rate. Now here's the riddle: is it possible that Dr. Lisa is better than Dr. Bart at both neurosurgery and bandaging, and yet have a lower overall success-rate? Counter-intuitively, the answer is ``yes". How is this possible? Well consider the following data set:
\vspace{15px}
\begin{center}
Dr. Lisa (total success rate: 80\%)\\
\vspace{5px}
\begin{tabular}{c|cc }
& \textbf{Neurosurgery} & \textbf{Bandaging} \\ 
\hline
\textbf{Successes} & 70 & 10 \\  
\textbf{Failures} & 20 & 0 \\
\hline
\textbf{Success Rate} & 78\% & 100\% 
\end{tabular}
\end{center}

\vspace{15px}
\begin{center}
Dr. Bart (total success rate: 83\%)\\
\vspace{5px}
\begin{tabular}{c|cc}
&\textbf{Neurosurgery} & \textbf{Bandaging} \\ 
\hline
\textbf{Successes} & 2 & 81 \\  
\textbf{Failures} & 8 & 9 \\
\hline
\textbf{Success Rate} & 20\% & 90\% 
\end{tabular}
\vspace{15px}
\end{center}

The lesson here is that sometimes if we just add up the totals and don't account for confounding variables, our results will be misleading. If you were to choose one of these doctors based on their overall success rate alone, you might choose Dr. Bart, despite the fact that Dr. Lisa is clearly a better doctor. This phenomenon is known as Simpson's paradox, named not after the hit television series but rather British statistician Edward H. Simpson who first described it in 1951 (\todo http://math.bme.hu/~marib/bsmeur/simpson.pdf). And since then, there have been many cases of analysts falling victim to it.

\subsection{Batting averages}
\todo One of the things that matters in baseball is a players batting average -- i.e. the proportion of times that a batter is successful. 300 means 30\%, which is very very good. Derek Jeter and David Justice. In 1995 and 1996 Justice has a better average than Jeter (CONFIRM). However if you just add them up, 
??????????????
\subsection{Racism on death row}
\todo Florida looked at all convicted murderers, and looked at what proportion were sentanced to death. They wanted to see if black people were sentenced to death more often then white people. Was there discrimination. Is there discrimination when it comes to the death penalty? They found that white defindents got sentenced to death 11\% of the time, but blakc people only got sent to death 7.9\% of the time. So this would seem to oppose the hypothesis. In fact, it seems that white people are being sent to death far more often. 

However lets look a little more deeply into the data and look at the victim's race, where you can see that this is really what matters. ???? Black people murder more black people, white people murder more white people, and those who murder white people get the death penalty more often. 

This is called a confounding variable







\section{The Secretary Problem}
Suppose you have applied to three jobs and are waiting to hear back. You are pretty sure that all three companies will give you an offer, but you do not know what salary they will offer you. And after each company presents their offer, you will have to decide whether to accept or reject it, and will not have time wait around to see what the next company is planning to offer you. What is the optimal strategy for maximizing your chances of accepting the best offer?
\\\\ 
As it turns out, the best strategy is to reject the first regardless of what they offer, and then accept any offer better than that. To illustrate this, let us suppose we have three companies $A$ (best offer), $B$ (second-best offer), and $C$ (worst offer). There are $3!=6$ orders in which they may call you. We will assume that all orders are equally likely, and consider what happens in each scenario:
\\\\
\allowdisplaybreaks
\begin{tabularx}{325pt}{cXc}

1.\ \textbf{ABC:} 
&
So far, our strategy is not off to a great start. We reject $A$ and are left disappointed by the remaining two offers. 
&
\raisebox{-0.75\height}{\includesvg[width=25px]{symbols/xcheck-red-x.svg}}
\\\\
2.\ \textbf{BAC:} 
&
This works out better a bit better. We reject $B$ and and then get the better offer $A$, which turns out to be the best.
&
\raisebox{-0.75\height}{\includesvg[width=25px]{symbols/xcheck-green-check.svg}}
\\\\
3.\ \textbf{BCA:} 
&
Here we reject $B$ and then $C$, and finally accept the best offer $A$.
&
\raisebox{-0.75\height}{\includesvg[width=25px]{symbols/xcheck-green-check.svg}}
\\\\
4.\ \textbf{CBA:} 
&
Here we do a little worse. By starting with $C$ we end up setting our standards too low and accept $B$ instead of holding out for the better offer
&
\raisebox{-0.75\height}{\includesvg[width=25px]{symbols/xcheck-red-x.svg}}
\\\\

5.\ \textbf{CAB:} 
&
But here, those low standards don't end up mattering, since the best offer happens to be the very next one.
&
\raisebox{-0.75\height}{\includesvg[width=25px]{symbols/xcheck-green-check.svg}}
\\\\

6.\ \textbf{ACB:} 
&
And once again, we end up the same problem we encountered in the first round, setting our standards too high and missing out on our best offer.
&
\raisebox{-0.75\height}{\includesvg[width=25px]{symbols/xcheck-red-x.svg}}

\end{tabularx}
\\\\

All in all, we end up getting the best offer half of the time.

\todo JUSTIFY THE GENERALIZATION

However let's generalize this this to $n$ people. (Answer $\nicefrac 1e$). This is often known as the 37\% rule. 


\section{Discrete Random Variables}
A \textbf{Discrete Random Variable} (DRV) $X$ is a function which map Events to integers. For instance, it could represent the face of a die we roll, or the number of heads we get flipping two coins. There is little in the way of properties which a DRV must satisfy, and it is perfectly possible for them to return negative integers as well as positive ones. For instance, if we measure the difference between the face of two dice or the total profits made from playing a casino game. We should however be able to define either a \textbf{Probability Mass Function} to a \textbf{Cumulative Distribution Function}

\subsection{Probability Mass Function}
Probability Mass Functions (PMFs) describe the probability of observing a given realization $x$ of a DRV $X$, and are often represented as either $P(X=x)$ or $f(x)$. We may define them in any way so long as they satisfy the following properties:
\begin{enumerate}
    \item For all $x\in \mathbb Z$, $f(x)\ge 0$
    \item $\displaystyle\sum_{x\in\mathbb Z} f(x)=1$
\end{enumerate}
Often PMFs are expressed with a table listing $(x, f(x))$ for all $x$-values such that $f(x)\ne 0$.

\begin{example}
    Suppose we toss a (fair) coin twice and let $X$ be a Random Variable representing the number of heads we observe. Define a PMF which describes this situation and show that it satisfies the necessary properties.

We may use
    \[
        \begin{array}{c|c}
            x & f(x) \\
            \hline
            0 & (\nicefrac 12)^2 \\
            1 & 2(\nicefrac 12)^2 \\
            2 & (\nicefrac 12)^2 
        \end{array}
    \]
We can see that $f(x)\ge 0$ for all values shown (and equal to zero for all values omitted), thus it satisfies Property 1. If we add the values up we see
\[
    \sum_{x\in\mathbb Z}f(x)=\frac 14 + \frac 12 + \frac 14 = 1
\]
\end{example}
\begin{notsofast}
    Do not confuse DRVs with their distributions! For instance, it is sometimes mistakenly assumed that the PMF of $2X$ is simply $2f(x)$. However this is not the case, and it is not even possible for $2f(x)$ to be \emph a PMF in the first place, let alone the PMF which correctly describes $2X$. Transforming DRVs is not so simple.
    \\\\
    For another common mistake, consider two DRVs $X$ and $Y$ which both have the same PMF. Are $X$ and $Y$ equal? Absolutely not! In fact, it may not even be possible to observe the same value from each of them at the same time. For a concrete example of this, suppose we flip three coins and let $X$ be the number of heads we observe, and let $Y$ be the number of tails we observe. Then the PMFs for $X$ and $Y$ are as follows:
    \[
        \begin{array}{ccc}
            \begin{array}{c|c}
                x & P(X=x) \\
                \hline
                0 & \nicefrac 18 \\
                1 & \nicefrac 38 \\
                2 & \nicefrac 38 \\
                3 & \nicefrac 18
            \end{array}
            & \hspace{100px} &
            \begin{array}{c|c}
                y & P(Y=y) \\
                \hline
                0 & \nicefrac 18 \\
                1 & \nicefrac 38 \\
                2 & \nicefrac 38 \\
                3 & \nicefrac 18
            \end{array}
        \end{array}
    \]
    Yet there is no scenario in which we can flip three coins and get the same number of heads as tails.
    \begin{proof}
        Since all coins must be either heads or tails, $x+y=3$. Suppose that $x=y$. Then we may write $2x=3$ and therefore $x$ (and by extension $y$) must not be an integer, which contradicts the real-world interpretation of $x$ and $y$.
    \end{proof}
\end{notsofast}

\subsection{The Cumulative Distribution Function}
Another way we can describe a Random Variable is using the Cumulative Distribution Function (CDF), often represented by $F(x)$. This describes, not the probability of observing $x$, but rather the probability of observing some value less than or equal to $x$. CDFs must satisfy the following properties:
\begin{enumerate}
    \item For all $x\in\mathbb Z$, $0\le F(x) \le 1$
    \item $F$ must be nondecreasing. In other words for any $x,y\in\mathbb Z$ such that $x<y$, $F(x)\le F(y)$
    \item For \emph{Discrete} Random Variables (we will discuss Continuous Random Variables in the next section) the $F$ is a step function
    \item $F$ is right continuous
\end{enumerate}
\todo

It is possible to determine the CDF from the PMF and vice versa using the following relation
\begin{lemma}
    \[
        f(x)=F(x)-F(x-1)
    \]
\end{lemma}\label{lem:cdf2pmf}


\begin{example}
    Suppose you roll 4 dice and let $X_i$ be the face of the $i$th die. Then let $X=\max\{X_1, X_2, X_3, X_4\}$. Find the PMF of $X$

\subsubsection{The Bad Way}
$X$ will return some value between 1 and 6 inclusive. For the maximum to be a $1$, we must have $X_1=X_2=X_3=X_4=1$, which has a probability of $(\nicefrac 1 6)^4$. Then the probability of the maximum being 2 is given by the probability that one of the 4 is 2 and the rest are either 1 or 2. Determining the probability of this Event would be tedious to calculate, and 3 4 and 5 will be just as tedious, so we will stop here hoping we have sufficiently made the point that this is a bad way to determine the PMF.

\subsubsection{The Good Way}
The CDF of $X_i$ for $i=1,2,3,4$ is
\[
    P(X_i\le x)=\frac x 6
\]
for all $x=1,2,...,6$ (for all other $x$-values it is zero) Then $X$ will return a value less than or equal to $x=1,2,...,6$ if and only if $X_1, X_2,X_3,X_4$ all return values less than or equal to $x$. Thus
\[
    P(X\le x)=\prod_{i=1}^4 P(X_i\le x)=\left(\frac x 6\right)^4
\]
Then using Lemma \ref{lem:cdf2pmf} we get
\[
    P(X=x)= \left(\frac x 6\right)^4-\left(\frac {x-1} 6\right)^4
\]
\end{example}



\subsection{Expectation}
\newcommand{\E}{\text E}
The \textbf{Expectation} of a Random Variable describes where the average observation of a Random Variable will tend towards when repeated many many times.
\begin{definition}[Expectation of a DRV]
Let $X$ be a DRV with PMF $f$. Then
    \[
        E(X)=\sum_{x\in\mathbb Z} x f(x)
    \]
    
\end{definition}
\begin{theorem}[Properties of Expectation]
    Let $X$ and $Y$ be two DRVs
    \begin{enumerate}
        \item $E(X+Y)=E(X)+E(Y)$
        \item For any transformation $g:\mathbb R\to\mathbb R$
        \[
            E\left[g(X)\right] = \sum_{x\in \mathbb Z} P(X=x) g(x)
        \]
    \end{enumerate}
    The proof of the first property may seem relatively straightforward. However it is trickier than one might expect, especially since it applies even when $X$ and $Y$ are Dependent. We will come back to it later.
    \todo PROOF OF SECOND
\end{theorem}

\begin{example}
    Find $E(X^2)$ where
    \[
        \begin{array}{c|c}
             x & f(x) \\
             \hline
             -1 & \nicefrac 13\\
             0 & \nicefrac 13 \\
             1 & \nicefrac 13
        \end{array}
    \]

    \subsubsection*{The Bad Way}
    Let $Y=X^2$. Then the PMF of $Y$ is as follows
    \[
        \begin{array}{c|c}
             y & f(y) \\
             \hline
             0 & \nicefrac 13 \\
             1 & \nicefrac 13 + \nicefrac 13
        \end{array}
    \]
    Then 
    \[
        \E(Y)=\frac 23
    \]
    \subsubsection*{The Good Way}
    \[
        E(X^2)=(-1)^2\times \frac 13 + 0^2\times \frac 13 + 1^2 \frac 13 = \frac 13
    \]
\end{example}

\subsection{The St. Petersberg Paradox}
There was ostensibly once a casino in St. Petersberg which offered a certain coin-flipping game. For some relatively large ticket-price, you could flip a coin. If the coin lands on heads, you would get your payout (starting out at a value much smaller than the ticket price). On the other hand, if the coin landed tails you got another flip with the payout doubled. The game could go on and on like this until the coin hits heads.
\begin{example}
    Calculate the Expected payout for the coin flip game of the St. Petersberg paradox, assuming that the initial payout is $\$2$.

    Let $X$ be the payout received from playing the coin game. Then the PMF of $X$ is
    \[
        \begin{array}{c|c}
             x & f(x) \\
             \hline
             2 & \nicefrac 12 \\
             4 & \nicefrac 14 \\
             16 & \nicefrac 1{16} \\
             \vdots & \vdots
        \end{array}
    \]
    From this we get
    \[
        E(X) = 2\times \frac 12 + 4 \times \frac 1 4 + \cdots = \infty
    \]
\end{example}
Thus this would not seem to be a particularly profitable game for a casino to run, since no matter what ticket price we set, the casino would in theory always lose money. So suppose we set a ticket price of $\$50$. Would playing be a good idea? Well, though the Expectation is technically infinite, we only have a 1 in 64 chance of actually winning any money. In other words, we could easily find ourselves over $\$3000$ in debt before seeing any winnings.  So clearly Expectation is not everything. This brings us to \textbf{Variance}.

\subsection{Variance}
\newcommand{\Var}{\text{Var}}
The Variance of a DRV is the average of the squared deviation from the mean. In other words
\begin{definition}[Variance of a DRV]
    Let $X$ be a DRV 
    \[
        \Var(X)=E[(X-E(X))^2]   
    \]
\end{definition}



\begin{theorem}[Properties of the Variance]
    Suppose $X$ and $Y$ are two Independent Random Variable, and $a$ and $b$ are constants
    \begin{enumerate}
        \item $\Var(X)=E(X^2)-[E(X)]^2$
        \item $\Var(a)=0$
        \item $\Var(aX+b)=a^2\Var(X)$
        \item $\Var(X+Y)=\Var(X)+\Var(Y)$
    \end{enumerate}
\end{theorem}

\begin{notsofast}
    The above Property 3 only applies if $X$ and $Y$ are Independent. For Dependent Random Variables, there are different rules.
\end{notsofast}


\subsection*{End-of-the-section Problems}
\begin{enumerate}
    \item Prove the linearity property of Expectation
    \item Show that $E[(X-E(X))^2]=E(X^2)-[E(X)]^2$
\end{enumerate}

\section{Named Distributions for DRVs}
\subsection{The Bernoulli Distribution}
Let $X$ be some $p\in[0,1]$ and consider some DRV with the following PMF
\[
    \begin{array}{c|c}
         x & f(x) \\
         \hline
         0 & 1-p \\
         1 & p
    \end{array}
\]
This is a valid PMF since $(1-p)+p=1$. It is a very simple distribution, but an important gateway to several other distributions -- important enough to warrant a name: \textbf{The Bernoulli Distribution}. We may state that a DRV $X$ has a Bernoulli distribution with a given $p$-value as $X\sim\text{Ber}(p)$.
\begin{theorem}[Properties of the Bernoulli Distribution]
    Let $X\sim\text{Ber}(p)$ for some $p\in[0,1]$. Then
    \begin{enumerate}
        \item $\E(X)=p$
        \item $\Var(X)=p(1-p)$
    \end{enumerate}

    The proof of this is left as an exercise to the reader.
\end{theorem}
\label{thm:propsbin}
\subsubsection{Indicator Random Variables}
Another important use for Bernoulli distribution is \textbf{Indicator Random Variables}. Suppose we have some Event $A$. The \textbf{Indicator Variable} for $A$ is as follows
\[
    I_A=\left\{\begin{array}{cl}
        1 & \text{if $A$ occurs}\\
        0 & \text{otherwise}
    \end{array}\right.
\]
This gives us a bridge from probabilities to averages.
\begin{example}[DeMontfort's Problem Revistied]
    Recall DeMontfrot's Problem. Let $X$ be the number of letters that end up in the correct envelope. Find $\E(X)$.

    Let $I_i$ be an Indicator Variable signalling hat the $i$th letter is in the correct envelope. We can then write
    \[
        X=\sum_{i=1}^n I_i
    \]
    Then by Linearity
    \[
         E(X)=\sum_{i=1}^n E(I_i) = \sum_{i=1}^n \frac 1n = 1
    \]
\end{example}
\begin{example}[Bluejays v. Redsox]
    Suppose the Bluejays are playing 17 games against the Redsox, and that the Bluejays have a 60\% chance of winning any given game. It may happen that the Redsox win one game, but then the Bluejays win the next, or that the Bluejays win one game, and the Redsox win the next. We call this a "turnover". Find the expected number of turnovers $\E(X)$.

    Let $I_i$ be an Indicator Random Varianle signalling that the $i$th game was a turnover. Since the first game cannot be a turnover but all others can, $i\in[2,17]\cap\mathbb Z$. Then
    \[
        \E(X)=\sum_{i=2}^{17} E(I_i)=\sum_{i=2}^{17} \frac {60}{100} \times \frac{40}{100}=16\times 0.48 = 7.68
    \]
\end{example}

\subsection{The Binomial Distribution}
\newcommand{\simiid}{\overset{iid}\sim}
Let $n$ be any positive integer and for $i=1,2,...,n$ let $I_i \simiid \text{Ber}(p)$ (where ``$\simiid$" means ``independent and identically distributed"). Then we say that $X=I_1+I_2+\cdots + I_n$ has a \textbf{Binomial Distribution} or $X\sim \text{Bin}(n,p)$.




\begin{theorem}[Properties of the Binomial Distribution]
    If $X=I_1+I_2+\cdots +I_n \sim \text{Bin}(n,p)$ for some $n\in\{1,2,...\}$ and $p\in[0,1]$. Then
    \begin{enumerate}
        \item $\forall x\in\{1,2,...,n\}, P(X=x)=\binom n x p^x(1-p)^{n-x}$
        \item $\E(X)=np$
        \item $\Var(X)=np(1-p)$
    \end{enumerate}
    \begin{proof}
    
        \begin{enumerate}
            \item[]
            \item There are $\binom nx$ possible ways for $x$ out of $n$ Bernoulli trials to return 1 and the remaining $(n-x)$ to return 0. The probability of $x$ Independent Bernoulli trials all returning 1 is $p^x$, and the probability of $n-x$ Independent Bernoulli trials all returning $0$ is $(1-p)^{n-x}$. Thus the probability of exactly $x$ out of $n$ Bernoulli trials returning 1 is $P(X=x)=\binom n x p^x(1-p)^{n-x}$. 
            \item By the Linearity Property
            \[
                E(X)=\sum_{i=1}^n E(I_i)=np
            \]
            \item 
            \[
                X^2=\sum_{i=1}^n I_i^2 + \sum_{j=1}^n\sum_{\substack{i=1 \\ j\ne i}}^n I_i I_j
            \]
            Notice that since $I_i$ will either return 1 or 0, in all cases $I_i^2=I_i$. Also since for any $i\ne j$, $I_i$ and $I_j$ are independent, and $I_i I_j$ will return 1 if and only if both $I_i$ and $I_j$ return 1 we get
            
            \[
                E(I_i I_j)=0+1\times P(I_i=1 \land I_j=1)=P(I_i=1)P(I_j=1)=p^2
            \]
            Therefore
            \[
                \E(X^2)=\sum_{i=1}^n p + \sum_{i=1}^n\sum_{\substack{i=1 \\ j\ne i}}^n p^2 = np+n(n-1)p^2
            \]
        \end{enumerate}
        Then
        \[
            \Var(X)=E(X^2)-[E(X)]^2= np+n(n-1)p^2 - n^2p^2=np(1-p)
        \]
    \end{proof}
    
    
\end{theorem}

\begin{example}
    Imagine a drunk man staggering home. He takes 20 steps, each in a random direction. What is the probability that he ends up 4 steps from where he started. We will assume that there are only two directions that he can go, and that each step has a fifty-fifty chance for going forwards versus backwards.

    We can cast this as a coin flipping problem. 
    \todo
\end{example}

\begin{example}
    A fair coin is tossed $n$ times. What is the probability that the first toss is a head given that exactly $r $ of the first $n$ tosses are heads.

    Let $A$ be the Event that the first toss was a head and let $B_{n,r}$ be the event that $r$ of the $n$ first tosses were heads. Then the quantity we are trying to find is equal to
    \[
        P(A|B_{n,r})=\frac{P(A\cap B_{n,r})}{P(B_{n,r})} 
    \] 
    $P(A\cap B_{n,r})$ is equivalent to the probability that the first toss is a head, and that $r-1$ of the remaining $n-1$ coins come up heads, both of which are Independent Events. Hence 
    \begin{align*}
        P(A\cap B_{n,r})
        &=P(A)P(B_{n-1,r-1})
        \\&=\frac 12 \times \binom{n-1}{r-1} \left(\frac 1 2 \right)^{n-1}
        \\&=\binom{n-1}{r-1} \left(\frac 1 2 \right)^{n}
        \\&=\frac{(n-1)!}{(r-1)!(n-r)!} \times \left(\frac 1 2 \right)^{n}
    \end{align*}
    We also have
    \[
        P(B_{n,r})=\binom{n}{r} \left(\frac 1 2 \right)^{n}=\frac{n!}{r!(n-r)!}\times \left(\frac 1 2 \right)^{n}
    \]
    Hence
    \[
        P(A|B_{n,r})=\frac{(n-1)(n-2)\cdots 1}{(r-1)(r-2)\cdots 1}\times \frac{r(r-1)\cdots 1}{n(n-1)\cdots 1}=\frac rn
    \]
\end{example}

\subsection{The Geometric Distribution}
    \newcommand{\geom}{\text{Geom}}
    We once again consider a sequence of independent Bernoulli trials, however we will now assume that there are an infinite number of them $I_1,I_2,...\simiid \text{Ber}(p)$. This time we let $X$ be the waiting time for the first success. For instance if $I_1$ is a success then $X$ returns 1. If $I_1$ is a failure and $I_2$ is a success then $X$ returns 2, etc. Or more generally if $I_x=1$ and $I_1=I_2=\cdots =I_{x-1}=0$, then $X=x$. In this case we say $X$ has a \textbf{Geometric Distribution} or
    \[
        X\sim\geom(p)
    \]
    The parameter $p$ here represents the probability of success on any given trial. Another useful quanitiy is the probability of failure on any given trial $q=1-p$
    \begin{definition}{Memorylessness}
        A DRV is said to be \textbf{Memoryless} if for all $s$ and $t$
        \[
            P(X>s+t\ |\ X>s)=P(X>t)
        \]
    \end{definition}
    \begin{theorem}[Properties of the Geometric Distribution]
        Let $X\sim \geom(p)$ for some $p\in [0,1]$ and $q=1-p$. Then
        \begin{enumerate}
            \item 
            $
                P(X=x)=\begin{cases}
                    q^{x-1}p & x\ge 1 \\
                    0 & \text{otherwise}
                \end{cases}
            $
            \item 
            $
                P(X\le x)=\begin{cases}
                    1-q^x & x\ge 1 \\
                    0 & \text{otherwise}
                \end{cases}
            $
            \item $\E(X)=\displaystyle\frac 1 p$
            \item $\Var(X)=\displaystyle \frac q {p^2}$
            \item The Geometric distribution is Memoryless (in fact it is the only Discrete Distribution with this property)
        \end{enumerate}
        \begin{proof}
            \begin{enumerate}
                \item[]
                \item $X$ will return $x$ when the first $x-1$ trials fail and the $x$th trial succeeds. The former probability is $q^{x-1}$ and the latter is $p$.
                \item We interpret $X>x$ to mean that that $I_1,I_2,...,I_x$ all failed (and $I_{x+1}, I_{x+2},...$ may or may not have failed or succeeded). Thus $P(X>x)=q^x$ and hence $P(X\le x)=1-q^x$.
                \item By the definition of Expectation of a DRV
                \[
                    \E(X)=\sum_{x=1}^\infty xq^{x-1}p=p+2qp+3q^2p+\cdots
                \]
                and therefore
                \[
                    q\E(X)=\sum_{x=1}^\infty xq^x p=\sum_{x=1}^\infty (x-1)q^{x-1} p =  qp+2q^2p+3q^3p+\cdots
                \]
                From these we get
                \[
                    p\E(X)=\E(X)-q\E(X)=\sum_{x=1}^\infty q^{x-1}p=\sum_{x=1}^\infty P(X=x)=1
                \]
                We then divide both sides by $p$ to get
                \[
                    \E(X)=\frac 1p
                \]
                \item \todo
                \item \todo
            \end{enumerate}
        \end{proof}
    \end{theorem}

    \begin{example}
        Jeopardy is a quiz show played by 3 people. Ifyou win Jeopardy you get to come back the next time. An average jeopardy player will have a 1 in 3 chance of winning a game. What is the probability of playing exactly 75 games.
        \[
            \left(\frac 1 3\right)^{74}\times \frac 2 3
        \]
    \end{example}

    \todo I defined $X$ as the waiting time for the first success, but some learned $Y$ as the number of failures before the first success. Some look at number of trials and some look at number of failures. In this class we use the first definition, however there is an easy conversion. $Y=X-1$. This chances the expected value to $q/p$ where $q=1-p$, but it does not change the variance
    
\subsection{Negative Binomial}
We can generalize the geometric distribution, and let $X$ be the number of Bernoulli trials before we get $k$ successes. Then we say $X$ has a \textbf{Negative Binomial Distribution} or $X\sim \text{NB}(p,k)$.
\begin{theorem}\textbf{Properties of the Negative Binomial Distribution}\\
    Let $X\sim \text{NB}(p,k)$. Then
    \begin{enumerate}
        \item $P(X=x)=\displaystyle{\left\{\begin{array}{ll}
                \binom {x-1}{k-1} p^{k-1}(1-p)^{x-k}p & \text{if $x \ge k$}\\
                0 & \text{otherwise}
            \end{array}\right.}$
        \item $\E(X)=k/p$
        \item $\Var(X)=\frac{k(1-p)}{p^2}$
    \end{enumerate}

    \begin{proof}
    \begin{enumerate}
        \item[]
        \item 
        \item We can split up the problem and let $T_i$ be the waiting time for the $\ell$th success where $i=1,2,...,k$. Then since $X$ is Memoryless, $T_i\simiid \text{Geom}(p)$, and $X=T_1+T_2+\cdots + T_k$. Hence
        \[
            \E(X)=\sum_{i=1}^k \E(T_i)=\frac k p
        \]
        \item Similarly, since they are independent
        \[
            \Var(X)=\sum_{i=1}^k \Var(T_i)=\frac{k(1-p)}{p^2}
        \]
    \end{enumerate}
    \end{proof}
\end{theorem}

Banach was a smoker. He had two matchboxes, one in each pocket, and he would choose between them by flipping a coin. 
Suppose each matchbox has $20$ matches. Suppose he finds that matchbox right is empty. What is the probability that there are 7 matches in his left pocket. Let $X$ be the number of matfches in the left matchbox. 

\[
    P(L=7)=\binom{33}{20}\todo
\]

\begin{example}
    Supose a fair coin is tossed repeatedly and independelnytl and $X$ is the waitng time for 2 heads. Find the $E(X|HTT)$ and $E(X|TTT)$.

    The remaining waiting time is still $\text{Geom}(\nicefrac 12)$. 

    For $TTT$ you wait for the first head and then the second head. The first 3 tosses have happened so we add that . And then its'

    \[
        E(X|TTT)=3+\frac{1}{\nicefrac 12}+\frac{1}{\nicefrac 12}=7
    \]
    \todo
        
\end{example}

\subsection{The Hypergeometric Distribution}
Let $r,n,N\in \{0,1,...\}$ such that $N\ge r$, and imagine some bag with $N$ balls in it. $r$ of the balls have the word ``Success" written on them, and the remaining $N-r$ balls have the word ``Failure" on them. Imagine we then reach our hand into the bag and pull out one ball, then another, and another, without replacement until we have $n$ balls. This scenario can be used as a metaphor for many things we might want to do in real life, and so we will name it and study it more in depth. Let $X$ represent the number of successes we draw. We call the distribution that $X$ has ``\textbf{Hypergeometric}" and say $X\sim \text{Hyp}(N,n,r)$. What is the probability $f(x)$ that $x$ of the balls we chose have ``Success" written on them? Let's first think about the bounds of $x$. The number of successes we draw can't be more than the number of balls we draw or greater than the number of successes there are in the bag. In other words, $x\le \min\{r,n\}$. There are similar rules about the number of failures. In other words $n-x\le \min\{N-r, n\}$, which is equivalent to $x\ge \max\{0, n+r-N\}$. Hence $f(x)$ only has non-zero values on the range $\max\{0, n+r-N\}\le x \le \min\{r,n\}$. To find these non-zero values, let us notice that there are $\binom Nn$ different ways we can pick out $n$ balls, and $\binom r x \binom {N-r}{n-x}$ different ways we can select $x$ successes and $n-x$ failures. Thus we define the Hypergeometric Distribution 




\begin{definition}[The Hypergeometric Distribution]
    For some $r,n,N\in \{0,1,...\}$ such that $N\ge r$ let $X\sim \text{Hyp}(N,n,r)$. Then the PMF of $X$ is 
    \[
        f(x)=\left\{\begin{array}{cl}
                \displaystyle\frac{\binom r x \binom {N-r}{n-x}}{\binom Nn} & \text{for $\max\{0, n+r-N\}\le x \le \min\{r,n\}$} \\\\
                0 & \text{otherwise}
            \end{array}\right.
    \]
\end{definition}
\begin{theorem}[Properties of the Hypergeometric Distribution]
    Let $X\sim \text{Hyp}(N,n,p)$. Then
    \begin{enumerate}
        \item $\E(x)=\todo$
        \item $\Var(x)=\todo$
    \end{enumerate}
\end{theorem}

\begin{example}[Loto 649]
    There is a very famous lottery in Canada called ``Loto 649". Players choose 6 numbers from 49 options. Later, the organizers of the game also choose 6 numbers from the same 49 randomly. If you as a player have all of those same numbers, you win first prize. With that in mind, what is the probability of winning Loto 649?

    We can use the bag metaphor with a success being the commission drawing a number you selected, and a failure being them drawing a number you didn't select. This gives us $N=49$ and $n=r=x=6$ giving us a probability of
    \[
        \displaystyle\frac{\binom 6 6 \binom {43}{0}}{\binom {49}{6}}=\frac 1{\binom {49} 6}=\frac 1 {13\,983\,816}
    \]
\end{example}
Suppose we have $N=10, r=5$. Then the probability of success in the first draw is $5/10=0.5$. Then our probability of success on our next draw is $4/9\approx 0.44$ or $5/9\approx 0.56$ depending on whether or not the first draw was a success. However, let's change these numbers to $N=10\,000$ and $r=5\,000$. Just as before the probability of success on the first draw is $5000/10000=0.5$, but then the probability of success on the next draw is $4999/9999\approx 0.5$ if the first draw was a success and $5000/9999\approx 0.5$ if the first draw was a success. Since the numbers are large here, each draw has very little effect on the probability of success on each draw, very similar to a series of Bernoulli trials. However this only goes so far. After the 500th draw, the probability of success may be very different. 
\begin{lemma}
    If $N$ is large and $n$ is small then we can approximate a Hypergeometric with $\text{Bin}(n,r/N$


    Let $H_{N,n}\sim\text{Hyp}(N,n,r)$ and let $B\sim\text{Bin}(r/N)$. Then for 
    \[
        \lim_{\substack{r\to\infty\\N\to\infty}} h_{N,n,r}(x)=b_{\frac r N}(x)
    \]
    Furthermore for any large fi
    \todo
    tl;dr The Hypergeometric is approximated by the binomail


    \begin{proof}


        We can model the ball scenario as a series of Bernoulli trials, albeit not independent ones, and not each with the same probability. Let $I_i$ be an Indicator Variable signalling that the $i$th draw was success. Let $I_i\sim\text{Ber}(p_i)$ for $i=1,2,...,n$ be an indicator variable signalling that the $i$th draw was a success. Let $r_i$ be the number of successes in the bag prior to the $i$th draw, and let $N_i$ be the total number of balls in the bag prior to the $i$th draw. Then $p_i=r_i/N_i$, and 
        \[
            p_{i+1}\in\left\{\frac{r_i-1}{N_i-1}, \frac{r_i}{N_i-1}\right\}
        \]
        and hence
        \[
            \frac{p_{i+1}}{p_i}\in\left\{\left(1-\frac{1}{r_i}\right)\frac{N_i}{N_i-1},\ \frac{N_i}{N_i-1}\right\}
        \]
        From this we get
        \[
            \left(1-\frac{1}{r}\right)^{i+1}\left(\frac N {N-1}\right)^{i+1} \le \frac{p_i}{p_1}\le \left(\frac N {N-1}\right)^{i+1}
        \]
        Hence by the squeeze theorem 
        \[
            \lim_{\substack{r\to\infty\\N\to\infty}}\frac{p_i}{p_1}=1
        \]
        implying that as $r$ and $N$ to infinity $p_1,p_2,...,p_n$ converge to a single value $p$. 

        
        The first draw has a probability of $\nicefrac r N$. This leaves $N-1$ balls in the bag and either $r$ or $r-1$ ``Success" balls depending on whether the first was a success or not. Hence the probability on the next draw will either be $r/(N-1)$ or $(r-1)/(N-1)$
    \end{proof}
    
\end{lemma}



\subsection{The Poisson Distribution}
We will now talk about arguably the most important discrete distribution of all: \textbf{The Poisson Distribution}.
\begin{definition}{The Poisson Distribution}
    $P(X=x)=\frac{e^{-\lambda \lambda^2}}{x!}$
\end{definition}
\begin{theorem}[Poisson Distribution]
    \begin{enumerate}
        \item $\forall x\in\{0,1,..\}, P(X=x)$
        \item $\E(X)=\lambda$
        \item $\Var(X)=\lambda$
    \end{enumerate}
\end{theorem}

So why is the Poisson DIstiuion so important. Let's usppose your coop job is very boring. You sit outside a bank and watch how many peiople are walking in. If someone walks in at a particular minute you put a check if not you don't. Your'= boss tells you to be more efficient and use 30 second intervals. 

Suppose we're loojing at eath quakes in LA in 6 months. Every instance, the change of an earth quake is samll. But if we look at every second in six months, that's a lot of trials. 

\subsubsection{Binomial Approximation}


\begin{lemma}
    Let $X\sim\text{Bin}(n,p)$ for $p\to 0$ and $n\to \infty$, but where $np=\lambda$. This is a Posson.

    \begin{proof}
    
    
        \[
            f(x)=\binom nx p^x(1-p)^{n-1}
        \]
        We can substitute $p=\lambda/n$. This gives us
        \[
            f(x)=\frac{n!}{x!(n-x)!} \frac{\lambda^x}{n^x}(1-\frac \lambda n)^n (1-\lambda /n)^{-x}
        \]

        We expand to
        \[
            \frac{n(n-1)\cdots()}{}
        \]
    \end{proof}
\end{lemma}

\begin{example}
    Suppose there are 200 people in a class and you want to find the probability that two of them were born on January 1st. We will assume there are no leap years or twins etc. 

    Let $X\sim\todo$ be the number of people born on January 1st. Then we have $n=200, p=1/365$
    \[
        P(X=2)=\binom {200}2 \frac 1 {365} \frac{364}{364}
    \]
    However we could alos use the Poisson approximation $X\sim Poi(200/365)$
\end{example}

\subsubsection{The Poisson Process}
Let $\{N(t)\}$ be a sequence of a rivals in continuous time. $N(t)$ is a poisson process if 
\begin{enumerate}
    \item $N(0)=0$
    \item The number of arrivals in $[0,t]$ is $Poi(\lambda t)$
    \item Arrivals in disjoint intervals are independent
\end{enumerate}
\begin{example}[Earthquakes]
    The number of earthquakes in an area follow a Poisson process with $\lambda = 6$ (earthquakes per year). Here you're doing lots of trials. You can think of each second as a trial. Find the probability that there are 4 earthquakes in 6 months. Then find the probability that in the next five years, two ofthem will have exactly five earthquakes. 

    \begin{enumerate}
        \item Let $N_t$ be the number of earthquakes in 6 months. Then $N_t\sim \text{Poi}(3)$. Thus we just need to find
        \[
            P(N_t=4)=e^{-3}\frac {3^4}{4!}
        \]
        \item Here we treat each year as a trial, giving us five trials. A success is getting exactly five earthquakes. The number of earthquakes in any one year follows a posson distribution and is
        \[
            p=e^{-5}\frac {6^5}{5!}
        \]
        And we plug this into a Binomial distribution
        \[
            p'=\binom 5 2 p^2(1-p)^{5-2}
        \]
    \end{enumerate}   
\end{example}

\subsection{End-of-the-Section Problems}
\begin{enumerate}
    \item Prove Theorem \ref{thm:propsbin}
    \item Suppose $n$ random strangers meet at a party. Assuming all birthdays are equally likely (and ignoring leap years, twins, etc.). Find the expected number of pairs of people who share a birthday. 
\end{enumerate}



\section{Continuous Random Variables}
Suppose you through a dart at a dartboard. What is the probab


Suppose you through a dart in a dartboard. The probability that you hit any one point is $0$, since there are an infinite number of points. $P(X=a)$ in all continuous distributions. We can use a CDF. Just as with DRVs it's 
PROPERTIES
\[
    F(X)=P(X\le x)
\]
however because the Point Probability is 0, we can also write
\[
    F(X)=P(X<x)
\]

We can't define a PMF, but we can define a \textbf{Probability Density Function}, which is defined such that for all $a$ and $b$
\[
    P(a<X<b)=\int_a^b f(x)dx
\]
\begin{notsofast}
    The density function does not return probabilities . Rather the area underneath the the PDF is a probability
\end{notsofast}
The PDF is the analog of the PMF.

Suppose $X$ was discrete. Yo
\[
    F(x)=\sum f(x)
\]
However in the continuous case
\[
    F(x)=\int_a^x f(x)dx
\]

\section{Moment Generating Functions}
Let $X$ be a Random Variable (either Discrete or Continuous). 
We define the \textbf{Moment} of $X$ as
\[
    M_X(t)=\E(e^{tx})
\]
Moments don't have

can be useful for finding different statistics such as variance and skewness.
This is just a mathematical tool with no real-world representation of things. 


So why do we care about this? There are three reasons
\begin{enumerate}
    \item We can use the Moment Generating Function to find Moments. The $n$th derivative of the Moment generating function at $t=0$ is the $n$th Moment of $X$. 
    \begin{proof}
        If we Taylor expand the defintion of $M_X$ we get
        \[
            M_X(t)=1+\frac{t\cdot \E(X)}{1!}+\frac{t^2\cdot \E(X^2)}{2!}+\cdots
        \]
        Then for some Higher Order Terms (H.O.T.)
        \[
            \frac{d^n M_X}{d t^n}=E(X^n)+t\times \text{H.O.T.}
        \]
        giving us
        \[
            M_X^{(n)}(0)=E(X^n)
        \]
    \end{proof}
    \item A given MGF uniquely identifies a Random Variable. 
    \begin{example}
        Suppose for some Random Variable $Y$
        \[
            M_Y(t)=\frac 23 + \frac 13 e^t
        \]
        What is $Y$
        
        $M_Y(t)$ is the MGF for a Ber($1/3$). Therefore that is the Distribution of $Y$
    \end{example}
    \item If $X$ and $Y$ are two Independent Randovm Variables, then
    \[
        M_{X+Y}(t)= M_X(t) M_Y(t)
    \]
\end{enumerate}

\begin{example}
    Find the MGF of $\text{Bin}(n,p)$
    
    Recall that for $I_i\simiid \text{Ber}(p)$ for $i=1,2,...,n$
    \[
        \sum_{i=1}^n I_i \sim \text{Bin}(n,p)
    \] We know that 
    \[
        \forall i\in\{1,2,...,n\}, M_{I_i}(t)=q+pe^t
    \]
    Hence the MGF for $\text{Bin}(n,p)$ is
    \[
        M_{I_1+I_2+\cdots + I_n}(t)=(q+pe^t)^n
    \]
\end{example}

\section{Named Continuous Distributions}
\subsection{The Uniform Distribution}
\todo 

\begin{example}

\end{example}
$\E(X)=\frac{a+b} 2$

Why do we care. First its a good introduction. But there is another very important reason. The universality property. 
\subsubsection{Universality of the Uniform Distributin}
Suppose you want to simulate a Continuous Random Variable. it's often easy to generate numbers on a $U(0,1)$, and from this we can generate any other distribution. Suppose we want to generate $n$ outcomes of a random variable $X$ with some CDF $F$. 
\begin{enumerate}
    \item[Step 1] Generate $n$ numbers from a $U(0,1)$, $u_1,u_2,...,u_n$.
    \item[Step 2] Then we just take $F^{-1}(u_i)$ for $i=1,2,...,n$. 
\end{enumerate}
\begin{example}
    Suppose your boss is interested in a distribution with a a PDF
    \[
        f(x)=\left\{\begin{array}{cc}
            e^{-x} & \text{if $x> 0$}\\
            0 & \text{otherwise}
            \end{array}\right.
    \]

    We can calclate the CDF $F(x)=1-e^x$ \todo\todo\todo
    We can ge\todo
\end{example}

$X\sim U(a,b)$
\[
    f(x)=\begin{cases}
        \frac 1 {b-a} & a\le x \le b \\
        0 & \text{otherwise}
    \end{cases}
\]

\begin{theorem}
    {The Universality Theorem}

    \todo
    
    \todo
\end{theorem}

\subsection{The Exponential Distribution}
$X\sim Exp(\lambda)$ if
\[
    f(x)=\begin{cases}
        \lambda e^{-\lambda x} & x \ge 0
        \\
        0 & \text{otherwise}
    \end{cases}
\]
Sometimes the Exponential distributoon uses $\mu=1/\lambda $ as its parammeter. We call $\mu$ the mean and $\lambda $ the rate. We get the CDF by integrating
\[
    F(x)=1-e^{-\lambda x}
\]

$\E(X)=\mu$
$\Var(X)=\mu^2$

The Exponential Distribution is the only Memoryless distribtuion.

Recall the definition of Memorylessness
\[
    P(X\ge s+t| X>s)=\frac{1-F(s+t)}{1-F(s)}
    =\frac{e^{-\lambda (s+t)}}{e^{-\lambda s}}
    =e^{-\lambda s}
    =P(X>t)
\]

\subsection{The Weibull Distribution}
Is the Exponential Distribtuion good for measuring human life. Suppose $s=60$
 and $t=40$, By the definition of Memorylessness.
 The Weibull Distribution is like the Exponential Distribution but with some decay.
 
\subsection{The Gamma and Erlang Distributions}
To talk about the \textbf{Gamma Distribution} we must talk about the \textbf{Gamma Function}
\[
    \Gamma(\alpha)=\int_0^\infty x^{\alpha - 1} e^x dx
\]
\begin{theorem}
    {Properties of the Gamma Function}
    \begin{enumerate}
        \item $\Gamma(\alpha+1)=\alpha\Gamma(\alpha)$
        \item For any integer $n$, $\Gamma(n+1)=n!$
    \end{enumerate}
    \
    \begin{enumerate}
        \item We can show this using integration by parts.
        \item Using the first property, we get that $\Gamma(n)=n\Gamma(n$\todo
    \end{enumerate}
\end{theorem}

From this we define the Gama Distribtuion
\begin{definition}
    We say $X\sim\Gamma(\alpha,\beta)$ if the PDF of $X$ is
    \[
        f(x)=\begin{cases}
            \todo
        \end{cases}
    \]
\end{definition}
We can show that $f(x)$ as defined above is a valid PDF
\begin{proof}
    If we let $t=x/\beta$, we get
    \[
        f(x)=\int_0^\int \frac{(t\beta)^{}\alpha-1 e^{-t}}{\beta^{\alpha} \Gamma(\alpha)}=\frac{\Gamma(\alpha)}{\Gamma(\alpha)}
    \]
\end{proof}

There are some notable special cases of the $\Gamma $ Distribtuin. If $\alpha = 1 $
\[
    f(x)=\frac 1\beta e^{-\frac x \beta}
\]
We should recognize this as the PDF for the Exponential distribution with mean $\beta$. In other words, the Exponential Distribution is simply a special case of the Gamma Distribution. When $\beta=1$ we get
\[
    f(x)=\frac{x^{\alpha-1}e^{-x}}{\Gamma(\alpha)}
\]
This function is interesting since the numerator is simple te integrand of the denominator. In cases where $\alpha$ is an integer, we may call the Gamma Distribtuion the Erlang distribution.
\begin{theorem}
    Let $k$ be any positive integer and let $X\sim \Gamma(\alpha,\beta)$. Then
    \[
        \E(X^k)=\beta^k \frac{\Gamma(\alpha+k)}{F(\alpha}
    \]
    
\end{theorem}

\begin{theorem}
    {Properties of the Gamma Distribution}
    \begin{enumerate}
        \item 
        \item 
        \item The MGF of $X$ is 
        \[
            M_X(t)=\frac 1 {(1-\beta t)^\alpha}
        \]
        %\item If $\alpha$ is an integer, then $X=Y_1+Y_2+\cdots Y_\alpha$ where $Y_i\simiid \text{Exp(\beta)}$ for $i=1,2,...,\alpha$.
    \end{enumerate}
    \begin{enumerate}
        \item This can be shown using Theorem \todo using $k=1$
        \item We can also find $\E(X^2)$ similarly which gives us
    \end{enumerate}
\end{theorem}

From this we see that the Erlang Distribtuion is the continous analogue of the Negative Binomial.



\subsection*{End-of-the-section Problems}
\begin{enumerate}
    \item Show that the MGF of $X\sim \text{Exp}(\mu)$ \todo
\end{enumerate}

\chapter{Multivariate and Conditional Distributions}

\chapter{Stochastic Process}



\end{document}
