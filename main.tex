\documentclass{report}

\usepackage{tcolorbox}
\usepackage{mdframed}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{svg}
\usepackage{times}
\usepackage{tabularx}

\usepackage{nicefrac}



\begin{document}

\setlength\parindent{0pt}

\newcommand{\todo}{TODO\quad}

\newmdtheoremenv[
    backgroundcolor=green!5!white,
    linecolor=green!75!black,
    font=\normalfont\bfseries,
    innertopmargin=6pt,
    splittopskip=\topskip,
]{example}{Example}[section]
\NewDocumentCommand{\solution}{o}{%
    \par\medskip% Add some vertical space
    \noindent% Remove paragraph indentation
    \IfValueTF{#1}{% If an optional argument is provided
        \textbf{\textup{#1:}}% Use the provided argument as the solution heading
    }{%
        \textbf{\textup{Solution:}}% Default solution heading
    }%
    \quad% Add some horizontal space
}

\newmdtheoremenv[
    backgroundcolor=blue!5!white,
    linecolor=blue!75!black,
    font=\normalfont\bfseries,
    innertopmargin=6pt,
    splittopskip=\topskip,
]{definition}{Definition}[section]

\newmdtheoremenv[
    backgroundcolor=yellow!5!white,
    linecolor=yellow!75!black,
    font=\normalfont\bfseries,
    innertopmargin=6pt,
    splittopskip=\topskip,
]{theorem}{Theorem}[section]

\newmdtheoremenv[
    backgroundcolor=yellow!5!white,
    linecolor=yellow!75!black,
    font=\normalfont\bfseries,
    innertopmargin=6pt,
    splittopskip=\topskip,
]{lemma}{Lemma}[section]

\newtcolorbox[auto counter,number within=section]{notsofast}[1][]{
    colback=red!5!white,
    colframe=red!75!black,
    fonttitle=\bfseries,
    title=\emph{Not So Fast!}
}


\tableofcontents

\chapter{Univariate Distributions and Generating Functions}

\section{Laws of Probability}
\subsection{Random Experiments}
Imagine you have have a coin and you flip it twice in a row. One of several things may happen. Perhaps you get two heads ($HH$), or two tails ($TT$), or perhaps some combination of the two ($HT$ or $TH$). We can list all the possibilities of what might happen, \emph{however} we do not know what \emph{will} happen. We will observe many situations like this this course and in real life, and we refer to them as \textbf{Random Experiments}. The \textbf{Sample Space} of a Random Experiment (often denoted as $S$ or $\Omega$) is the set of all possible outcomes we may observe. For instance:
\[
    \Omega=\{HH,HT,TH,TT\}
\]
\begin{notsofast}
The Sample Space is not unique! We may define it differently depending on what properties we are interested in. For instance, suppose we do not care about the order of the coins but simply how many heads we observe. In this case we could define the Sample Space as
\[
    \Omega=\{0,1,2\}
\]
which is equally valid.
\end{notsofast}
\subsection{Events and Probability}
We are often interested in subsets of the Sample Space. For instance: what are the different ways we can get the same coin twice?
\[
    \{HH,TT\}\subseteq \Omega
\]
An \textbf{Events} is any subset of a Sample Space. Since Events are just subsets, the union and intersect of any two Events is also an Event. We say that Events $A$ and $B$ are \textbf{Mutually Exclusive} if $A\cup B=\emptyset$. From here we get the definition of \textbf{Probability}:
\begin{definition}[Probability]
A Probability is any function which maps Events to $\mathbb R$ and satisfies the following axioms:
\begin{enumerate}
    \item 
    \begin{enumerate}
        \item $P(\emptyset) = 0$
        \item $P(\Omega)=1$
    \end{enumerate}
    \item If $A_1, A_2,...$ are Mutually Exclusive Events, then
    \[
        P\left(\ \bigcup_{i=1}^\infty A_i\ \right) = \sum_{i=1}^\infty P(A_i)
    \]
\end{enumerate}
\end{definition}
\label{def:probability}
From this one small definition, we can derive the entire field of probability theory, including the following lemmas:

\begin{lemma}
    For any Event $A$ with compliment $\bar A$, $P(\bar A)=1-P(A)$
\begin{proof}
    Since $A$ and $\bar A$ are compliments of one another, they are mutually exclusive and satisfy
    \[
        A\cup \bar A = \Omega
    \]
    and hence
    \[
        P(A)+P(\bar A) = P(\Omega) = 1
    \]
    giving us
    \[
        P(\bar A)=1-P(A)
    \]
\end{proof}
\end{lemma}

\begin{lemma}
    For any two Events $A$ and $B$ such that $A\subseteq B$, $P(A)\le P(B)$.
    
    \begin{proof}
    Consider the Event $B\backslash A$, which is mutually exclusive to $A$. Since $A\subseteq B$ we may express $B$ as
    \[
        B=A\cup (B\backslash A)
    \]
    giving us
    \[
        P(B)=P(A)+P(B\backslash A)
    \]
    It can be shown that the probability of any Event is greater than or equal to zero (this is left as an exercise to the reader), and therefore $P(B)\le P(A)$.
    \end{proof}
\end{lemma}

One special case of Definition \ref{def:probability} is known as \textbf{The Classical Definition of Probability}. \todo CLASSICAL DEFINITION

\subsection{End-of-the-section Problems}\label{sec:problaw_eosp}

\begin{enumerate}
    \item Which of the following has the highest probability?
    \begin{enumerate}
        \item Observing at least one six in 6 rolls of a fair die
        \item At least two sixes in 12 rolls of a fair die
        \item At least three sixes in 18 rolls of a fair die
    \end{enumerate}
    \item Suppose $n$ random strangers meet at a party. Assuming all birthdays are equally likely (and ignoring leap years, twins, etc.) show that if $n\ge 23$ then the probability of two people sharing the same birthday is greater than \nicefrac 12.
    \item Suppose two fair dice are rolled. What is the probability of getting at least one six?
    \item\textbf{(The Secretary's Problem)}
    Suppose you have applied to three jobs and are waiting to hear back. You are pretty sure that all three companies will give you an offer, but you do not know what salary they will offer you. And after each company presents their offer, you will have to decide whether to accept or reject it, and will not have time wait around to see what the next company is planning to offer you. What is the optimal strategy for maximizing your chances of accepting the best offer?
    \item \textbf{(DeMontfort's Problem)} 
    \begin{enumerate}
        \item Suppose you have 4 envelopes and 4 letters which you place in the envelopes at random such that each envelope contains exactly one letter. Find the probability that at least one of of the 4 letters ends up in the correct envelope.
        \item Find the general solution for $n$ letters and $n$ envelopes, and show what happens when $n\to\infty$.
    \end{enumerate}
    \item \textbf{(Simpson's Paradox)} Suppose we have two doctors (we will call them Dr. Bart and Dr. Lisa) each performing two operations (for instance: neurosurgery and bandaging a wound). Both doctors perform each surgery with some success rate. Now here's the riddle: is it possible that Dr. Lisa is better than Dr. Bart at both neurosurgery and bandaging, and yet have a lower overall success-rate? How might this strategy generalize to $n$ offers.
\end{enumerate}

\section{Conditional Probability}
Let $A$ and $B$ be two Events. Then the probability of $A$ given that $B$ has occurred, denoted b y $P(A\vert B)$ is given by
\[
    P(A\vert B)=\frac{P(A\cap B)}{P(B)}
\]
We often call $P(A)$ the \textbf{Prior Probability} and $P(A\vert B)$ the \textbf{Posterior Probability}.
\begin{notsofast}
    A common mistake is to assume that $P(A\vert B)=P(B\vert A)$, however this is not the case. This mistake is often known as ``the Prosecutor's Fallacy". We will discuss the high-profile case from where this name originated later in the section.
\end{notsofast}

\begin{figure}
    \centering
    \includesvg{figures/dependence.svg}
    \caption{An illustration of Conditional Probability. The Prior Probability $P(A)$ is $\nicefrac 5{10}$, however the Posterior Probability $P(A|B)$ is $\nicefrac 35$}
    \label{fig:dependence}
\end{figure}

\subsection{LOTP and Bayes Rule}
The definition of $P(A\vert B)$ is deceptively simple, but it has wide-ranging consequences. But first, we must establish yet more terminology: A \textbf{Partition} of $\Omega$ is any set of Mutually Exclusive Events $B_1,B_2,...,B_n$ such that $B_1\cup B_2\cup\cdots \cup B_n=\Omega$.

\begin{theorem}[Law of Total Probability] Let $B_1,B_2,...,B_n$ be some Partition of $\Omega$. Then for any Event $A$
\[
    P(A)=\sum_{i=1}^n P(A\vert B_i) P(B_i)
\]

\begin{proof}
    Since $B_1,B_2,...,B_n$ form a Partition of $\Omega$, we can decompose $A$ into the following union of Mutually Exclusive Events:
    \[
        A=(A\cap B_1)\cup (A\cap B_2) \cup \cdots \cup (A\cap B_n)
    \]
    \begin{center}
        \includesvg{figures/lotp.svg}
    \end{center}
    Then by Definition \ref{def:probability}
    \[
        P(A)=P(A\cap B_1)+ P(A\cap B_2)+ \cdots + P(A\cap B_n)
    \]
\end{proof}
\end{theorem}

\begin{example}
    Suppose you have two coins. One is fair, whereas the other one has $P(H)=\nicefrac 34$. One coin is picked at random and tossed 3 times. Find the probability that all three tosses return \emph{Heads}.

    \solution
    Let $F$ be the Event that a fair coin is chosen out of the two, and let $H^3$ be the probability of observing 3 heads. If we assume that there is an equal chance of choosing each coin, then the Prior Probability of observing $F$ and $\bar F$ are $P(F)=P(\bar F)=\nicefrac 12$, and the Posterior Probabilities of $H^3$ are
    \[
        P(H^3 | F) = \left(\frac 12\right)^3;\quad P(H^3 | \bar F) = \left(\frac 34\right)^3
    \]
    Since $F$ and $\bar F$ are a Partition of the Sample Space, we may then use the Law of Total Probability to find
    \[
        P(H^3) = P(H^3|F)P(F)+P(H^3|\bar F)P(\bar F)= \left(\frac 12\right)^3\times \frac 12 + \left(\frac 34\right)^3\times\frac 12
    \]
    or around $27\%$.
\end{example}
    

\begin{theorem}[Bayes Rule]
    Let $B_1,B_2,...,B_n$ be some Partition of $\Omega$. Then for any Event $A$ and any $i\in \{1,2,...,n\}$:
    \[
        P(B_i\vert A) = \frac{P(A\vert B_i)P(B_i)}{P(A)}
    \]
    or equivalently, using the Law of Total Probability
    \[
        P(B_i\vert A) = \frac{P(A\vert B_i)P(B_i)}{\sum_{i=j}^n P(A|B_j)P(B_j)}
    \]
    
    The proof of this is left as an exercise to the reader. 
\end{theorem}

\begin{example}
    Suppose you have two coins. One is fair, whereas the other one has $P(H)=\nicefrac 34$. You pick one coin at random and flip it three times. All three tosses turn up heads. What is the probability that the fair coin was chosen?

    We are trying to find $P(F|H^3)$. By Bayes Rule
    \[
        P(F | H^3) = \frac {P(H^3| F)P(F)}{P(H^3)} = \frac{(\nicefrac 12)^3\times \nicefrac 12}{\left(\nicefrac 12\right)^3\times \nicefrac 12 + \left(\nicefrac 34\right)^3\times\nicefrac 12}
    \]
    or around $23\%$.
\end{example}

\subsection{The Prosecutor's Fallacy}
Sally Clark was a British woman who lost both of her two children (apparently) to SIDS the late 1990s. This raised suspicion leading her to be tried for murder. The prosecution's case hinged on Sir Roy Meadows' argument that SIDS is very rare, only affecting around 1 in every 8500 children. Therefore the probability of two children dying of SIDS $(1/8500)^2$. In other words, there is a $99.9999986\%$ chance that she is guilty, which clearly goes beyond any reasonable doubt one could have, and so the jury convicted her (though this conviction was later overturned). There are two issues with Meadows' argument:
First of all, the it assumes that the two deaths are are Independent Events. But are they? Perhaps there is a genetic component to SIDS which both children had, or something in the shared environment. Hence the probability of two children in the same family dying may not be all that much higher than the probability of just one. This is an important point, however on its own it does little do dispel the evidence against Mrs. Clark, since even the probability of only one child dying is rather low. In fact, had the courts tried Mrs. Clark after her only her first child, they could have made a similar argument and claimed there was a $99.988\%$ chance she was guilty based on that alone. So then by this logic, should all cases of SIDS be treated not only with suspicion but as outright proof of fowl play? Clearly this would be preposterous, and it is here we begin to really see the cracks in the Meadows' argument.
\\\\
Let $E$ be the probability of having two children die of SIDS, and let $I$ be the probability of the mother being innocent. If we do assume that each child dying is an Independent Event, then it is true that $P(E|I)=1/8500^2$. However, Meadows claim conflated this with $P(I|E)$. These quantities are not necessarily equal, however they are related, so it is worth asking: what \emph{can} we learn about $P(I|E)$ from $P(E|I)$. Using Bayes Rule, 
\[
    P(I|E)=\frac{P(E|I)P(I)}{P(E)}=\frac 1 {72\,250\,000}\times \frac{P(I)}{P(E)}
\]

\todo finish????

\todo https://forensicstats.org/blog/2018/02/16/misuse-statistics-courtroom-sally-clark-case/

\section{Statistical Independence}
\subsection{Independence with two Events}
Two Events $A$ and $B$ are \textbf{Independent} if they do not have any ``information" about the other. In other words knowing that $B$ has occurred does not change the probability of $A$ having occurred. We can express this symbolically as follows
\[
    P(A|B)=P(A)
\]
or equivalently:
\[
    P(A\cap B)=P(A)P(B)
\]
\begin{notsofast}
    Independence is \emph{not} the same as Mutual Exclusivity. In fact if $A$ and $B$ are Mutually Exclusive then they cannot be independent. 
    \begin{proof}
        Suppose $A$ and $B$ are two Mutually Exclusive Events and we know that $B$ has occurred. Then we also know $A$ must not have occurred. In other words, knowing $B$ has occurred has given us information about the probability of $A$ (namely that the probability is zero). 
    \end{proof}
\end{notsofast}

\begin{lemma}
Let $A$ and $B$ be Independent Events. Then the following pairs are also Independent:
\begin{enumerate}
    \item $A$ and $\bar B$
    \item $\bar A$ and $B$
    \item $\bar A$ and $\bar B$
\end{enumerate}

\begin{proof}
    \begin{enumerate}
        \item[]
        \item\todo
        \item\todo
        \item \todo
    \end{enumerate}
\end{proof}
\end{lemma}

\subsection{Independence with more than two Events}

\subsection{Conditional Independence}


Two events are \textbf{Conditionally Independent} given $C$ if 
\[
    P(A\cap B | C)= P(A|C)P(B|C)
\]

\todo






\section{Simpson's Paradox}
Let us return to one of the problems we considered at the end of \ref{sec:problaw_eosp}: Suppose we have two doctors (we will call them Bart and Lisa) each performing two operations (for instance: neurosurgery and bandaging a wound). Both doctors perform each surgery with some success rate. Now here's the riddle: is it possible that Dr. Lisa is better than Dr. Bart at both neurosurgery and bandaging, and yet have a lower overall success-rate? Counter-intuitively, the answer is ``yes". How is this possible? Well consider the following data set:
\vspace{15px}
\begin{center}
Dr. Lisa (total success rate: 80\%)\\
\vspace{5px}
\begin{tabular}{c|cc }
& \textbf{Neurosurgery} & \textbf{Bandaging} \\ 
\hline
\textbf{Successes} & 70 & 10 \\  
\textbf{Failures} & 20 & 0 \\
\hline
\textbf{Success Rate} & 78\% & 100\% 
\end{tabular}
\end{center}

\vspace{15px}
\begin{center}
Dr. Bart (total success rate: 83\%)\\
\vspace{5px}
\begin{tabular}{c|cc}
&\textbf{Neurosurgery} & \textbf{Bandaging} \\ 
\hline
\textbf{Successes} & 2 & 81 \\  
\textbf{Failures} & 8 & 9 \\
\hline
\textbf{Success Rate} & 20\% & 90\% 
\end{tabular}
\vspace{15px}
\end{center}

The lesson here is that sometimes if we just add up the totals and don't account for confounding variables, our results will be misleading. If you were to choose one of these doctors based on their overall success rate alone, you might choose Dr. Bart, despite the fact that Dr. Lisa is clearly a better doctor. This phenomenon is known as Simpson's paradox, named not after the hit television series but rather British statistician Edward H. Simpson who first described it in 1951 (\todo http://math.bme.hu/~marib/bsmeur/simpson.pdf). And since then, there have been many cases of analysts falling victim to it.

\subsection{Batting averages}
\todo One of the things that matters in baseball is a players batting average -- i.e. the proportion of times that a batter is successful. 300 means 30\%, which is very very good. Derek Jeter and David Justice. In 1995 and 1996 Justice has a better average than Jeter (CONFIRM). However if you just add them up, 
??????????????
\subsection{Racism on death row}
\todo Florida looked at all convicted murderers, and looked at what proportion were sentanced to death. They wanted to see if black people were sentenced to death more often then white people. Was there discrimination. Is there discrimination when it comes to the death penalty? They found that white defindents got sentenced to death 11\% of the time, but blakc people only got sent to death 7.9\% of the time. So this would seem to oppose the hypothesis. In fact, it seems that white people are being sent to death far more often. 

However lets look a little more deeply into the data and look at the victim's race, where you can see that this is really what matters. ???? Black people murder more black people, white people murder more white people, and those who murder white people get the death penalty more often. 

This is called a confounding variable







\section{The Secretary Problem}
Suppose you have applied to three jobs and are waiting to hear back. You are pretty sure that all three companies will give you an offer, but you do not know what salary they will offer you. And after each company presents their offer, you will have to decide whether to accept or reject it, and will not have time wait around to see what the next company is planning to offer you. What is the optimal strategy for maximizing your chances of accepting the best offer?
\\\\ 
As it turns out, the best strategy is to reject the first regardless of what they offer, and then accept any offer better than that. To illustrate this, let us suppose we have three companies $A$ (best offer), $B$ (second-best offer), and $C$ (worst offer). There are $3!=6$ orders in which they may call you. We will assume that all orders are equally likely, and consider what happens in each scenario:
\\\\
\allowdisplaybreaks
\begin{tabularx}{325pt}{cXc}

1.\ \textbf{ABC:} 
&
So far, our strategy is not off to a great start. We reject $A$ and are left disappointed by the remaining two offers. 
&
\raisebox{-0.75\height}{\includesvg[width=25px]{symbols/xcheck-red-x.svg}}
\\\\
2.\ \textbf{BAC:} 
&
This works out better a bit better. We reject $B$ and and then get the better offer $A$, which turns out to be the best.
&
\raisebox{-0.75\height}{\includesvg[width=25px]{symbols/xcheck-green-check.svg}}
\\\\
3.\ \textbf{BCA:} 
&
Here we reject $B$ and then $C$, and finally accept the best offer $A$.
&
\raisebox{-0.75\height}{\includesvg[width=25px]{symbols/xcheck-green-check.svg}}
\\\\
4.\ \textbf{CBA:} 
&
Here we do a little worse. By starting with $C$ we end up setting our standards too low and accept $B$ instead of holding out for the better offer
&
\raisebox{-0.75\height}{\includesvg[width=25px]{symbols/xcheck-red-x.svg}}
\\\\

5.\ \textbf{CAB:} 
&
But here, those low standards don't end up mattering, since the best offer happens to be the very next one.
&
\raisebox{-0.75\height}{\includesvg[width=25px]{symbols/xcheck-green-check.svg}}
\\\\

6.\ \textbf{ACB:} 
&
And once again, we end up the same problem we encountered in the first round, setting our standards too high and missing out on our best offer.
&
\raisebox{-0.75\height}{\includesvg[width=25px]{symbols/xcheck-red-x.svg}}

\end{tabularx}
\\\\

All in all, we end up getting the best offer half of the time.

\todo JUSTIFY THE GENERALIZATION

However let's generalize this this to $n$ people. (Answer $\nicefrac 1e$). This is often known as the 37\% rule. 


\section{Discrete Random Variables}
A key step when performing any kind of experiment is measurement. Measurement can mean a lot of things, for some experiments it may involve high-tech equipment and analyzers, in others it may involve reading through surveys filled out by participants in a study, or it may be as simple as looking at a coin to see if it came up heads or not. In all cases however, the principals are the same: we perform an experiment (i.e. flipping a coin 3 times), observe some outcome $\omega$ (i.e. heads, tails, heads), and quantify something about that outcome (i.e. counting the number of times heads came up). In this sense measurements can be thought of as mappings from the Sample Space of an Experiment to numbers.

\begin{figure}
    \centering
    \includesvg{figures/measurement.svg}
    \caption{Caption}
    \label{fig:my_label}
\end{figure}

\newpage


observe some outcome $\omega$ from the experiment's sample 












Suppose we flip a coin 10 times and we are interested in the number of times we get heads. We could let $P(A)$ be the Probability of getting no heads, $P(B)$ be the Probability of getting one head, and so on. However this can obviously become tedious very quickly. Situations like this come up a lot in statistics, so it is worth creating a framework to deal with them more efficiently. This brings us to the concept of a \textbf{Random Variable}. Unlike traditional variables, the value of a Random Variable is probabilistic in nature. Thus statements made about Random Variables are not true or false but instead have some Probability of being true and some Probability of being false. This makes them perfect for describing Random Experiments. 
\begin{example}
    Suppose we flip a coin 10 times and let $X$ be a Random Variable representing the number of times we get heads. Find the truth-probability for each of the following statements
    \begin{enumerate}
        \item $X\ge 5$
        \item $X^2\ne 0$ 
        \item $X=37$
        \item $X\in\{\pi, \nicefrac{10} 3, \sqrt {-1}\}$
    \end{enumerate}
    \solution
    \begin{enumerate}
        \item There are $2^{10}$ possible sequences we could get in this Experiment, and out of them $\binom {10} 5$ include exactly 5 heads, $\binom {10} 6$ include exactly 6 heads, and so on. Thus
        \[
            P(X\ge 5) = \frac 1 {2^{10}} \sum_{i=5}^{10} \binom {10} i
        \]
        \item $X^2$ is only equal to zero when no coins come up heads. Thus
        \[
            P(X^2\ne 0) = \frac{2^{10}-1}{2^{10}}
        \]
        \item Since we only flip the coin 10 times, it's not possible to get 37 heads. Thus
        \[
            P(X=37)=0
        \]
        \item Since $X$ represents a number of coins it is not possible for it to have a non-integer value. Thus
        \[
            P(X\in\{\pi, \nicefrac{10} 3, \sqrt {-1}\})=0
        \]
    \end{enumerate}
\end{example}
In this section we will consider Random Variables which are known to be integer-valued, i.e. Random Variables with the property $P(X\in\mathbb Z)=1$. This class of Random Variable is known as a \textbf{Discrete Random Variable} or DRV.

\subsection{Probability Mass Functions (PMFs)}
Now that we have defined what a DRV is the question arises of how to express them. The simplest and most obvious way is to simply try and make a table of all the values the DRV might take along with their respective probabilities. For instance
\begin{example}
    Suppose we toss a (fair) coin twice and let $X$ be a DRV representing the number of heads we observe. Describe $X$ using a table
    \solution
    The possible values of $X$ and their respective probabilities are as follows
        \[
            \begin{array}{c|c}
                x & P(X=x) \\
                \hline
                0 & (\nicefrac 12)^2 \\
                1 & 2(\nicefrac 12)^2 \\
                2 & (\nicefrac 12)^2 
            \end{array}
        \]
\end{example}
This can have some drawbacks however. For example suppose $X$ has a large or even infinite number of possible values. One way to address this is to define a function $f: \mathbb Z\to \mathbb R$ which gives the probability of $X$ having a given value $x$. This function $f(x)$ is known as a \textbf{Probability Mass Function} or PMF. We will use PMFs as our basis for describing DRVs. However since the range of a PMF is meant to be a Partition of a Sample Space, we must follow some ground rules when creating them in order to have a DRV that makes sense:
\begin{definition}[Probability Mass Function]
    Let $f:\mathbb Z\to \mathbb R$. Then $f$ is a valid PMF if it satisfies the following axioms 
    \begin{enumerate}
        \item $\forall x\in\mathbb Z, 0\le f(x) \le 1$
        \item $\displaystyle\sum_{x\in\mathbb Z} f(x)= 1$
    \end{enumerate}
\end{definition}

\begin{example}
    Suppose we toss a (fair) coin twice and let $X$ be a DRV representing the number of heads we observe. Describe $X$ using a PMF
    \solution
    \[
        P(X=x)=f(x)=\begin{cases}
             \frac {\binom 2 x} 4 & 0\le x \le 2
            \\
            0 & \text{otherwise}
        \end{cases}
    \]
    We can show that this is a valid PMF.
    \begin{proof}
        \begin{enumerate}
            \item[]
            \item Exhaustive checking each $x=0,1,2$ we see that all $f$-values fall between 0 and 1. 
            \item Adding them up we then get 1.
        \end{enumerate}
    \end{proof}
\end{example}
\begin{notsofast}
    Do not confuse DRVs with their distributions! For instance, it is sometimes mistakenly assumed that the PMF of $2X$ is simply $2f(x)$. However this is not the case, and it is not even possible for $2f(x)$ to be \emph a PMF in the first place, let alone the PMF which correctly describes $2X$. Transforming DRVs is not so simple.
    \\\\
    For another common mistake, consider two DRVs $X$ and $Y$ which both have the same PMF. Are $X$ and $Y$ equal? Absolutely not! In fact, it may not even be possible to observe the same value from each of them at the same time. For a concrete example of this, suppose we flip three coins and let $X$ be the number of heads we observe, and let $Y$ be the number of tails we observe. Then the PMFs for $X$ and $Y$ are as follows:
    \[
        \begin{array}{ccc}
            \begin{array}{c|c}
                x & P(X=x) \\
                \hline
                0 & \nicefrac 18 \\
                1 & \nicefrac 38 \\
                2 & \nicefrac 38 \\
                3 & \nicefrac 18
            \end{array}
            & \hspace{100px} &
            \begin{array}{c|c}
                y & P(Y=y) \\
                \hline
                0 & \nicefrac 18 \\
                1 & \nicefrac 38 \\
                2 & \nicefrac 38 \\
                3 & \nicefrac 18
            \end{array}
        \end{array}
    \]
    Yet there is no scenario in which we can flip three coins and get the same number of heads as tails.
    \begin{proof}
        Since all coins must be either heads or tails, $x+y=3$. Suppose that $x=y$. Then we may write $2x=3$ and therefore $x$ (and by extension $y$) must not be an integer, which contradicts the real-world interpretation of $x$ and $y$.
    \end{proof}
\end{notsofast}

\subsection{Cumulative Distribution Functions}
Another way we can describe a Random Variable is using the Cumulative Distribution Function (CDF), often represented by $F(x)=P(X\le x)$. 
\begin{theorem}[Properties of the CDF]
    Let $X$ be some DRV defined by a PMF $f(x)=P(X=x)$ and let $F(x)=P(X\le x)$. Then
    \begin{enumerate}
        \item $F(x)=\displaystyle\sum_{k=-\infty}^x f(x)$
        \item $F(x)$ is a unique identifier for the probability distribution of $X$
        \item $f(x)=F(x)-F(x-1)$
        \item $\forall x\in\mathbb Z$, $0\le F(x) \le 1$
        \item $F$ is nondecreasing. In other words for any $x,y\in\mathbb Z$ such that $x<y$, $F(x)\le F(y)$
    \end{enumerate}
    \begin{proof}
        \begin{enumerate}
            \item[]
            \item This is trivial given the definition of $F$ and $f$
            \item Then since we can derive $F$ from $f$ that must mean that $F$ is unique
            \item From (1) we get
            \[
                F(x)-F(x-1)=\sum_{k=-\infty}^x f(k) - \sum_{k=-\infty}^{x-1} f(k) =f(k)
            \]
            \item This follows from the fact that $F$ represents a probability 
            \item Let $x\in\mathbb Z$ and let $y=x+s$ for some positive integer $s$. Using (1) we write
            \[
                F(x+s)-F(x)=\sum_{k=-\infty}^{x+s} f(k) - \sum_{k=-\infty}^{x} f(k) = \sum_{k=1}^{x+s} f(k) 
            \]
            which is nonnegative since it is the sum of nonnegative values
        \end{enumerate}
        
    \end{proof}
\end{theorem}
\begin{theorem}
    Let $F:\mathbb Z\to\mathbb R$ be a step function with the following properties
    \begin{enumerate}
        \item $F$ is right-continuous
        \item $F$ is nondecreasing
        \item $\forall x\in\mathbb Z$, $0\le F(x) \le 1$
        \item $\displaystyle\lim_{x\to\infty} F(x)=1$
        \item $\displaystyle\lim_{x\to -\infty} F(x)=0$
    \end{enumerate}
    Then $F$ is a valid CDF.
    \begin{proof}
        Let $f(x)=F(x)-F(x-1)$. Since $F$ is nondecreasing, $f(x)\ge 0$ for all $x\in\mathbb Z$. Furthermore, since $0\le F(x) \le 1$ for all $x\in\mathbb Z$ we know that $f(x)\le 1$. Let $a,b\in\mathbb Z$ such that $a<b$. Then
        \begin{align*}
            \sum_{x=a}^b f(x) & = \sum_{x=a}^b\left[F(x)-F(x-1)\right]
            \\                & = \left[\sum_{x=a}^b F(x)\right] - \left[\sum_{x=a-1}^{b-1} F(x)\right]
            \\                & = F(b)-F(a-1)+\left[\sum_{x=a}^{b-1} F(x)\right] - \left[\sum_{x=a}^{b-1} F(x)\right]
            \\                & = F(b)-F(a-1)
        \end{align*}
        From this we get
        \[
            \sum_{x\in\mathbb Z} f(x)=\lim_{b\to \infty} F(b) - \lim_{a\to -\infty} F(a-1)=1
        \]
        Thus $f$ satisfies all the properties of a PMF.
    \end{proof}
\end{theorem}


So why bother with this entirely different way to define a DRV? The PMF and CDF can both be useful in different situations. For instance

\begin{example}
    Suppose you roll 4 dice and let $X_i$ be the face of the $i$th die. Then let $X=\max\{X_1, X_2, X_3, X_4\}$. Find the PMF of $X$

    \solution[The Bad Way]
    $X$ will return some value between 1 and 6 inclusive. For the maximum to be a $1$, we must have $X_1=X_2=X_3=X_4=1$, which has a probability of $(\nicefrac 1 6)^4$. Then the probability of the maximum being 2 is given by the probability that one of the 4 is 2 and the rest are either 1 or 2. Determining the probability of this Event would be tedious to calculate, and 3 4 and 5 will be just as tedious, so we will stop here hoping we have sufficiently made the point that this is a bad way to determine the PMF.
    
    \solution[The Good Way]
    The CDF of $X_i$ for $i=1,2,3,4$ is
    \[
        P(X_i\le x)=\frac x 6
    \]
    for all $x=1,2,...,6$ (for all other $x$-values it is zero) Then $X$ will return a value less than or equal to $x=1,2,...,6$ if and only if $X_1, X_2,X_3,X_4$ all return values less than or equal to $x$. Thus
    \[
        P(X\le x)=\prod_{i=1}^4 P(X_i\le x)=\left(\frac x 6\right)^4
    \]
    Then
    \[
        P(X=x)= \left(\frac x 6\right)^4-\left(\frac {x-1} 6\right)^4
    \]
\end{example}

\subsection{Expectation of a DRV}
\newcommand{\E}{\textup{E}}
The \textbf{Expectation} of a Random Variable describes where the average observation of a Random Variable will tend towards when repeated many many times.
\begin{definition}[Expectation of a DRV]
Let $X$ be a DRV with PMF $f(x)$. Then
    \[
        E(X)=\sum_{x\in\mathbb Z} x f(x)
    \]
    
\end{definition}
\begin{theorem}[Properties of Expectation]
    Let $X$ and $Y$ be two DRVs
    \begin{enumerate}
        \item $E(X+Y)=E(X)+E(Y)$
        \item For any transformation $g:\mathbb R\to\mathbb R$
        \[
            E\left[g(X)\right] = \sum_{x\in \mathbb Z} P(X=x) g(x)
        \]
    \end{enumerate}
    The proof of the first property may seem relatively straightforward. However it is trickier than one might expect, especially since it applies even when $X$ and $Y$ are Dependent. We will come back to it later.
    \todo PROOF OF SECOND
\end{theorem}

\begin{example}
    Find $E(X^2)$ where
    \[
        \begin{array}{c|c}
             x & f(x) \\
             \hline
             -1 & \nicefrac 13\\
             0 & \nicefrac 13 \\
             1 & \nicefrac 13
        \end{array}
    \]

    \solution[The Bad Way]
    Let $Y=X^2$. Then the PMF of $Y$ is as follows
    \[
        \begin{array}{c|c}
             y & f(y) \\
             \hline
             0 & \nicefrac 13 \\
             1 & \nicefrac 13 + \nicefrac 13
        \end{array}
    \]
    Then 
    \[
        \E(Y)=\frac 23
    \]
    \solution[The Good Way]
    \[
        E(X^2)=(-1)^2\times \frac 13 + 0^2\times \frac 13 + 1^2 \frac 13 = \frac 13
    \]
\end{example}

\subsection{The St. Petersberg Paradox}
There was ostensibly once a casino in St. Petersberg which offered a certain coin-flipping game. For some relatively large ticket-price, you could flip a coin. If the coin lands on heads, you would get your payout (starting out at a value much smaller than the ticket price). On the other hand, if the coin landed tails you got another flip with the payout doubled. The game could go on and on like this until the coin hits heads.
\begin{example}
    Calculate the Expected payout for the coin flip game of the St. Petersberg paradox, assuming that the initial payout is $\$2$.
    \solution
    Let $X$ be the payout received from playing the coin game. Then the PMF of $X$ is
    \[
        \begin{array}{c|c}
             x & f(x) \\
             \hline
             2 & \nicefrac 12 \\
             4 & \nicefrac 14 \\
             16 & \nicefrac 1{16} \\
             \vdots & \vdots
        \end{array}
    \]
    From this we get
    \[
        E(X) = 2\times \frac 12 + 4 \times \frac 1 4 + \cdots = \infty
    \]
\end{example}
Thus this would not seem to be a particularly profitable game for a casino to run, since no matter what ticket price we set, the casino would in theory always lose money. 

So suppose you were offered a chance to play this game with a ticket price of $\$50$. Would you take take it? Though the Expectation is technically infinite, we only have a 1 in 64 chance of actually winning any money. In other words, we could easily find ourselves over $\$3\,000$ in debt before seeing any winnings.  So clearly Expectation is not everything. This brings us to \textbf{Variance}.

\subsection{Variance}
\newcommand{\Var}{\textup{Var}}
The Variance of a Random Variable is the average of the squared deviation from the mean. In other words
\begin{definition}[Variance of a Random Variable]
    Let $X$ be a Random Variable
    \[
        \Var(X)=E[(X-E(X))^2]   
    \]
\end{definition}


\begin{theorem}[Properties of the Variance]
    Suppose $X$ and $Y$ are two Independent DRVs, and $a$ and $b$ are constants
    \begin{enumerate}
        \item $\Var(X)=E(X^2)-[E(X)]^2$
        \item $\Var(a)=0$
        \item $\Var(aX+b)=a^2\Var(X)$
        \item $\Var(X+Y)=\Var(X)+\Var(Y)$
    \end{enumerate}
    \todo PROOF
\end{theorem}

\begin{notsofast}
    The above Property 3 only applies if $X$ and $Y$ are Independent. For Dependent Random Variables, there are different rules.
\end{notsofast}


\subsection{End-of-the-section Problems}
\begin{enumerate}
    \item Prove the linearity property of Expectation
    \item Show that $E[(X-E(X))^2]=E(X^2)-[E(X)]^2$
\end{enumerate}

\section{Named Discrete Distributions}
\subsection{The Bernoulli Distribution}
\newcommand{\ber}{\textup{Ber}}
Let $X$ be some $p\in[0,1]$ and consider a DRV $X$ with the following PMF
\[
    \begin{array}{c|c}
         x & f(x) \\
         \hline
         0 & 1-p \\
         1 & p
    \end{array}
\]
This is a valid PMF since $(1-p)+p=1$. It is a very simple distribution, but an important gateway to several other distributions -- important enough to warrant a name: \textbf{The Bernoulli Distribution}. We may state that a DRV $X$ has a Bernoulli distribution with a given $p$-value as $X\sim\ber(p)$.
\begin{theorem}[Properties of the Bernoulli Distribution]
    Let $X\sim\ber(p)$ for some $p\in[0,1]$. Then
    \begin{enumerate}
        \item $\E(X)=p$
        \item $\Var(X)=p(1-p)$
    \end{enumerate}

    The proof of this is left as an exercise to the reader.
\end{theorem}
\label{thm:propsbin}
\subsubsection{Indicator Random Variables}
Another important use for Bernoulli distribution are \textbf{Indicator Random Variables}. Suppose we have some Event $A$. The \textbf{Indicator Variable} for $A$ is as follows
\[
    I_A=\begin{cases}
        1 & \text{if $A$ occurs}\\
        0 & \text{otherwise}
    \end{cases}
\]
This gives us a bridge from probabilities to averages.
\begin{example}[DeMontfort's Problem Revistied]
    Recall DeMontfrot's Problem. Let $X$ be the number of letters that end up in the correct envelope. Find $\E(X)$.
    \solution
    Let $I_i$ be an Indicator Variable signalling hat the $i$th letter is in the correct envelope. We can then write
    \[
        X=\sum_{i=1}^n I_i
    \]
    Then by Linearity
    \[
         E(X)=\sum_{i=1}^n E(I_i) = \sum_{i=1}^n \frac 1n = 1
    \]
\end{example}
\begin{example}[Bluejays v. Redsox]
    Suppose the Bluejays are playing 17 games against the Redsox, and that the Bluejays have a 60\% chance of winning any given game. It may happen that the Redsox win one game, but then the Bluejays win the next, or that the Bluejays win one game, and the Redsox win the next. We call this a ``turnover". Find the expected number of turnovers $\E(X)$.
    \solution
    Let $I_i$ be an Indicator Random Variable signalling that the $i$th game was a turnover. Since the first game cannot be a turnover but all others can, $i\in[2,17]\cap\mathbb Z$. Then
    \[
        \E(X)=\sum_{i=2}^{17} E(I_i)=\sum_{i=2}^{17} \frac {60}{100} \times \frac{40}{100}=16\times 0.48 = 7.68
    \]
\end{example}

\subsection{The Binomial Distribution}
\newcommand{\simiid}{\overset{iid}\sim}
\newcommand{\bin}{\textup{Bin}}
Let $n$ be any positive integer and for $i=1,2,...,n$ let $I_i \simiid \ber(p)$ (where ``$\simiid$" means ``independent and identically distributed"). Then we say that $X=I_1+I_2+\cdots + I_n$ has a \textbf{Binomial Distribution} or $X\sim \bin(n,p)$.




\begin{theorem}[Properties of the Binomial Distribution]
    If $X=I_1+I_2+\cdots +I_n \sim \bin(n,p)$ for some $n\in\{1,2,...\}$ and $p\in[0,1]$. Then
    \begin{enumerate}
        \item $P(X=x)=\begin{cases}
            \displaystyle\binom n x p^x(1-p)^{n-x} & 0\le x \le n \\\\
            0 & \text{otherwise}
        \end{cases}$
        \item $\E(X)=np$
        \item $\Var(X)=np(1-p)$
    \end{enumerate}
    \begin{proof}
    
        \begin{enumerate}
            \item[]
            \item There are $\binom nx$ possible ways for $x$ out of $n$ Bernoulli trials to return 1 and the remaining $n-x$ to return 0. The probability of $x$ Independent Bernoulli trials all returning 1 is $p^x$, and the probability of $n-x$ Independent Bernoulli trials all returning $0$ is $(1-p)^{n-x}$. Thus the probability of exactly $x$ out of $n$ Bernoulli trials returning 1 is $P(X=x)=\binom n x p^x(1-p)^{n-x}$. 
            \item By the Linearity Property
            \[
                E(X)=\sum_{i=1}^n E(I_i)=np
            \]
            \item 
            \[
                X^2=\sum_{i=1}^n I_i^2 + \sum_{j=1}^n\sum_{\substack{i=1 \\ j\ne i}}^n I_i I_j
            \]
            Notice that since $I_i$ will either return 1 or 0, in all cases $I_i^2=I_i$. Also since for any $i\ne j$, $I_i$ and $I_j$ are independent, and $I_i I_j$ will return 1 if and only if both $I_i$ and $I_j$ return 1 we get
            \[
                E(I_i I_j)=0+1\times P(I_i=1 \land I_j=1)=P(I_i=1)P(I_j=1)=p^2
            \]
            Therefore
            \[
                \E(X^2)=\sum_{i=1}^n p + \sum_{i=1}^n\sum_{\substack{i=1 \\ j\ne i}}^n p^2 = np+n(n-1)p^2
            \]
            giving us
            \[
                \Var(X)=E(X^2)-[E(X)]^2= np+n(n-1)p^2 - n^2p^2=np(1-p)
            \]
        \end{enumerate}
    \end{proof}
    
    
\end{theorem}

\begin{example}
    Imagine a drunk man staggering home. He takes 20 steps, each in a random direction. What is the probability that he ends up 4 steps from where he started. We will assume that there are only two directions that he can go, and that each step has a fifty-fifty chance for going forwards versus backwards.

    We can cast this as a coin flipping problem. 
    \todo
\end{example}

\begin{example}
    A fair coin is tossed $n$ times. What is the probability that the first toss is a head given that exactly $r $ of the first $n$ tosses are heads.

    Let $A$ be the Event that the first toss was a head and let $B_{n,r}$ be the event that $r$ of the $n$ first tosses were heads. Then the quantity we are trying to find is equal to
    \[
        P(A|B_{n,r})=\frac{P(A\cap B_{n,r})}{P(B_{n,r})} 
    \] 
    $P(A\cap B_{n,r})$ is equivalent to the probability that the first toss is a head, and that $r-1$ of the remaining $n-1$ coins come up heads, both of which are Independent Events. Hence 
    \begin{align*}
        P(A\cap B_{n,r})
        &=P(A)P(B_{n-1,r-1})
        \\&=\frac 12 \times \binom{n-1}{r-1} \left(\frac 1 2 \right)^{n-1}
        \\&=\binom{n-1}{r-1} \left(\frac 1 2 \right)^{n}
        \\&=\frac{(n-1)!}{(r-1)!(n-r)!} \times \left(\frac 1 2 \right)^{n}
    \end{align*}
    We also have
    \[
        P(B_{n,r})=\binom{n}{r} \left(\frac 1 2 \right)^{n}=\frac{n!}{r!(n-r)!}\times \left(\frac 1 2 \right)^{n}
    \]
    Hence
    \[
        P(A|B_{n,r})=\frac{(n-1)(n-2)\cdots 1}{(r-1)(r-2)\cdots 1}\times \frac{r(r-1)\cdots 1}{n(n-1)\cdots 1}=\frac rn
    \]
\end{example}

\subsection{The Geometric Distribution}
    \newcommand{\geom}{\textup{Geom}}
    Let's once again consider a sequence of independent Bernoulli trials, however we will now assume that there are an infinite number of them $I_1,I_2,...\simiid \ber(p)$. This time we let $X$ be the waiting time for the first success. For instance if $I_1$ is a success then $X$ returns 1. If $I_1$ is a failure and $I_2$ is a success then $X$ returns 2, etc. Or more generally if $I_x=1$ and $I_1=I_2=\cdots =I_{x-1}=0$, then $X=x$. In this case we say $X$ has a \textbf{Geometric Distribution} or
    \[
        X\sim\geom(p)
    \]
    The parameter $p$ here represents the probability of success on any given trial. Another useful quanitiy is the probability of failure on any given trial $q=1-p$
    \begin{definition}{Memorylessness}
        A DRV is said to be \textbf{Memoryless} if for all $s$ and $t$
        \[
            P(X>s+t\ |\ X>s)=P(X>t)
        \]
    \end{definition}
    \begin{theorem}[Properties of the Geometric Distribution]
        Let $X\sim \geom(p)$ for some $p\in [0,1]$ and $q=1-p$. Then
        \begin{enumerate}
            \item 
            $
                P(X=x)=\begin{cases}
                    0 & x \le 0
                    \\
                    q^{x-1}p & x\ge 1
                    
                \end{cases}
            $
            \item 
            $
                P(X\le x)=\begin{cases}
                    0 & x \le 0
                    \\
                    1-q^x & x\ge 1
                \end{cases}
            $
            \item $\E(X)=\displaystyle\frac 1 p$
            \item $\Var(X)=\displaystyle \frac q {p^2}$
            \item The Geometric distribution is Memoryless (in fact it is the only Discrete Distribution with this property)
        \end{enumerate}
        \begin{proof}
            \begin{enumerate}
                \item[]
                \item $X$ will return $x$ when the first $x-1$ trials fail and the $x$th trial succeeds. The former probability is $q^{x-1}$ and the latter is $p$.
                \item We interpret $X>x$ to mean that that $I_1,I_2,...,I_x$ all failed (and $I_{x+1}, I_{x+2},...$ may or may not have failed or succeeded). Thus $P(X>x)=q^x$ and hence $P(X\le x)=1-q^x$.
                \item By the definition of Expectation of a DRV
                \[
                    \E(X)=\sum_{x=1}^\infty xq^{x-1}p=p+2qp+3q^2p+\cdots
                \]
                and therefore
                \[
                    q\E(X)=\sum_{x=1}^\infty xq^x p=\sum_{x=1}^\infty (x-1)q^{x-1} p =  qp+2q^2p+3q^3p+\cdots
                \]
                From these we get
                \[
                    p\E(X)=\E(X)-q\E(X)=\sum_{x=1}^\infty q^{x-1}p=\sum_{x=1}^\infty P(X=x)=1
                \]
                We then divide both sides by $p$ to get
                \[
                    \E(X)=\frac 1p
                \]
                \item \todo
                \item \todo
            \end{enumerate}
        \end{proof}
    \end{theorem}
    Be advised that not all mathematicians are consistent with their precise definition of a Geometric distribution, and though this is the definition we will use in this course, there is another popular one which measures the number of failures \emph{before} the first success. This subtly changes many of the above formulas. However we can easily convert between the two systems. If we let $Y$ be the number of failures before the first success, then $Y=X-1$.
    \begin{example}
        Jeopardy is a popular quiz show wherein three contestants face off each episode in a battle of knowledge. Out of the three, only the first-place winner gets to come back and play again in the next episode. Yet one contestant, Ken Jennings, is famous for having won 74 consecutive games. Given that, by definition, an average player has a 1 in 3 chance of winning on any given episode, what is the probability of an average player matching Ken Jenning's luck exactly?
        \solution
        \[
            \left(\frac 1 3\right)^{74}\times \frac 2 3
        \]
    \end{example}
\subsection{The Negative Binomial Distribution}
\newcommand{\nbin}{\textup{NB}}
We can generalize the Geometric Distribution, and let $X$ be the number of Bernoulli trials required to get $k$ successes. Then we say $X$ has a \textbf{Negative Binomial Distribution} or $X\sim \nbin(p,k)$.
\begin{theorem}[Properties of the Negative Binomial Distribution]
    Let $X\sim \nbin(p,k)$ for some $p\in[0,1]$ and $k\in\{1,2,...\}$. Then
    \begin{enumerate}
        \item $P(X=x)=\begin{cases}
                \displaystyle\binom {x-1}{k-1} p^{k}(1-p)^{x-k} & x \ge k\\\\
                0 & \text{otherwise}
            \end{cases}$
        \item $\E(X)=\displaystyle\frac k p$
        \item $\Var(X)=\displaystyle\frac{k(1-p)}{p^2}$
    \end{enumerate}

    \begin{proof}
    \begin{enumerate}
        \item[]
        \item In order for $X=x$
        \item We can split up the problem and let $T_i$ be the waiting time for the $\ell$th success where $i=1,2,...,k$. Then since $X$ is Memoryless, $T_i\simiid \text{Geom}(p)$, and $X=T_1+T_2+\cdots + T_k$. Hence
        \[
            \E(X)=\sum_{i=1}^k \E(T_i)=\frac k p
        \]
        \item Similarly, since they are independent
        \[
            \Var(X)=\sum_{i=1}^k \Var(T_i)=\frac{k(1-p)}{p^2}
        \]
    \end{enumerate}
    \end{proof}
\end{theorem}

Banach was a smoker. He had two matchboxes, one in each pocket, and he would choose between them by flipping a coin. 
Suppose each matchbox has $20$ matches. Suppose he finds that matchbox right is empty. What is the probability that there are 7 matches in his left pocket. Let $X$ be the number of matches in the left matchbox. 

\[
    P(L=7)=\binom{33}{20}\todo
\]

\begin{example}
    Supose a fair coin is tossed repeatedly and independelnytl and $X$ is the waitng time for 2 heads. Find the $E(X|HTT)$ and $E(X|TTT)$.

    The remaining waiting time is still $\text{Geom}(\nicefrac 12)$. 

    For $TTT$ you wait for the first head and then the second head. The first 3 tosses have happened so we add that . And then its'

    \[
        E(X|TTT)=3+\frac{1}{\nicefrac 12}+\frac{1}{\nicefrac 12}=7
    \]
    \todo
        
\end{example}

\subsection{The Hypergeometric Distribution}
\newcommand{\hyp}{\textup{Hyp}}
Let $r,n,N\in \{0,1,...\}$ such that $N\ge r$, and imagine some bag with $N$ balls in it. $r$ of the balls have the word ``Success" written on them, and the remaining $N-r$ balls have the word ``Failure" on them. Imagine we then reach our hand into the bag and pull out one ball, then another, and another, without replacement until we have $n$ balls. This scenario can be used as a metaphor for many things we might want to do in real life, and so we will name it and study it more in depth. Let $X$ represent the number of successes we draw. We call the distribution that $X$ has ``\textbf{Hypergeometric}" and say $X\sim \hyp(N,n,r)$. What is the probability $f(x)$ that $x$ of the balls we chose have ``Success" written on them? Let's first think about the bounds of $x$. The number of successes we draw can't be more than the number of balls we draw or greater than the number of successes there are in the bag. In other words, $x\le \min\{r,n\}$. There are similar rules about the number of failures. In other words $n-x\le \min\{N-r, n\}$, which is equivalent to $x\ge \max\{0, n+r-N\}$. Hence $f(x)$ only has non-zero values on the range $\max\{0, n+r-N\}\le x \le \min\{r,n\}$. To find these non-zero values, let us notice that there are $\binom Nn$ different ways we can pick out $n$ balls, and $\binom r x \binom {N-r}{n-x}$ different ways we can select $x$ successes and $n-x$ failures. Thus we define the Hypergeometric Distribution using the following PMF
\[
    f(x)=\begin{cases}
        \displaystyle\frac{\binom r x \binom {N-r}{n-x}}{\binom Nn} & \max\{0, n+r-N\}\le x \le \min\{r,n\} \\\\
        0 & \text{otherwise}
    \end{cases}
\]
\begin{theorem}[Properties of the Hypergeometric Distribution]
    Let \\ $X \sim \hyp(N,n,p)$. Then
    \begin{enumerate}
        \item $\E(x)=\todo$
        \item $\Var(x)=\todo$
    \end{enumerate}
\end{theorem}

\begin{example}[Loto 649]
    There is a very famous lottery in Canada called ``Loto 649". Players choose 6 numbers from 49 options. Later, the organizers of the game also choose 6 numbers from the same 49 randomly. If you as a player have all of those same numbers, you win first prize. With that in mind, what is the probability of winning Loto 649?

    We can use the bag metaphor with a success being the commission drawing a number you selected, and a failure being them drawing a number you didn't select. This gives us $N=49$ and $n=r=x=6$ giving us a probability of
    \[
        \displaystyle\frac{\binom 6 6 \binom {43}{0}}{\binom {49}{6}}=\frac 1{\binom {49} 6}=\frac 1 {13\,983\,816}
    \]
\end{example}
Suppose we have $N=10, r=5$. Then the probability of success in the first draw is $5/10=0.5$. Then our probability of success on our next draw is $4/9\approx 0.44$ or $5/9\approx 0.56$ depending on whether or not the first draw was a success. However, let's change these numbers to $N=10\,000$ and $r=5\,000$. Just as before the probability of success on the first draw is $5000/10000=0.5$, but then the probability of success on the next draw is $4999/9999\approx 0.5$ if the first draw was a success and $5000/9999\approx 0.5$ if the first draw was a success. Since the numbers are large here, each draw has very little effect on the probability of success on each draw, very similar to a series of Bernoulli trials. However this only goes so far. After the 500th draw, the probability of success may be very different. 
\begin{lemma}
    If $N$ is large and $n$ is small then we can approximate a Hypergeometric with $\bin(n,r/N$


    Let $H_{N,n}\sim\hyp(N,n,r)$ and let $B\sim\bin(r/N)$. Then for 
    \[
        \lim_{\substack{r\to\infty\\N\to\infty}} h_{N,n,r}(x)=b_{\frac r N}(x)
    \]
    Furthermore for any large fi
    \todo
    tl;dr The Hypergeometric is approximated by the binomail


    \begin{proof}


        We can model the ball scenario as a series of Bernoulli trials, albeit not independent ones, and not each with the same probability. Let $I_i$ be an Indicator Variable signalling that the $i$th draw was success. Let $I_i\sim\ber(p_i)$ for $i=1,2,...,n$ be an indicator variable signalling that the $i$th draw was a success. Let $r_i$ be the number of successes in the bag prior to the $i$th draw, and let $N_i$ be the total number of balls in the bag prior to the $i$th draw. Then $p_i=r_i/N_i$, and 
        \[
            p_{i+1}\in\left\{\frac{r_i-1}{N_i-1}, \frac{r_i}{N_i-1}\right\}
        \]
        and hence
        \[
            \frac{p_{i+1}}{p_i}\in\left\{\left(1-\frac{1}{r_i}\right)\frac{N_i}{N_i-1},\ \frac{N_i}{N_i-1}\right\}
        \]
        From this we get
        \[
            \left(1-\frac{1}{r}\right)^{i+1}\left(\frac N {N-1}\right)^{i+1} \le \frac{p_i}{p_1}\le \left(\frac N {N-1}\right)^{i+1}
        \]
        Hence by the squeeze theorem 
        \[
            \lim_{\substack{r\to\infty\\N\to\infty}}\frac{p_i}{p_1}=1
        \]
        implying that as $r$ and $N$ to infinity $p_1,p_2,...,p_n$ converge to a single value $p$. 

        
        The first draw has a probability of $\nicefrac r N$. This leaves $N-1$ balls in the bag and either $r$ or $r-1$ ``Success" balls depending on whether the first was a success or not. Hence the probability on the next draw will either be $r/(N-1)$ or $(r-1)/(N-1)$
    \end{proof}
    
\end{lemma}



\subsection{The Poisson Distribution}
\newcommand{\pois}{\textup{Pois}}
We will now talk about arguably the most important discrete distribution of all: \textbf{The Poisson Distribution}, which is defined using the following PMF
\[
    f(x)=\begin{cases}
        \displaystyle\frac{e^{-\lambda }\lambda^x}{x!} & x\ge 0
        \\\\
        0 & \text{otherwise}
    \end{cases}
\]
for some parameter $\lambda\in(0,\infty)$ often called the ``mean". This distribution is often used for counting the number of times some event happens during some period of time. For instance, the number of earthquakes that occur in Los Angeles during a 6-month period might be a good candidate for the Poisson Distribution. Whether or not a particular phenomenon follows a Poisson distribution is an empirical question, and cannot be answered mathematically. However the the Poisson distribution is often a great starting point. Typically we want phenomena which have a large number of trials each with a very low probability of success. For instance in the case of earthquakes, it is possible but very unlikely that an earthquake will occur at any given second.
\begin{theorem}[Properties of the Poisson Distribution]
    Let $X\sim\pois(\lambda)$ and let $Y\sim\pois(\mu)$ for some $\lambda,\mu\in(0,\infty)$ such that $X$ and $Y$ are independent. Then
    \begin{enumerate}
        \item $\E(X)=\lambda$
        \item $\Var(X)=\lambda$
        \item $X+Y\sim\pois(\lambda+\mu)$
    \end{enumerate}
    \todo PROOF
\end{theorem}

One useful application of the Poisson distribution is for approximating Binomial distributions with large $n$ and small $p$


\begin{lemma}
    Let $X\sim\bin(n,p)$ for $p\to 0$ and $n\to \infty$, but where $np=\lambda$. This is a Posson.
    \todo
    \begin{proof}
    
    
        \[
            f(x)=\binom nx p^x(1-p)^{n-1}
        \]
        We can substitute $p=\lambda/n$. This gives us
        \[
            f(x)=\frac{n!}{x!(n-x)!} \frac{\lambda^x}{n^x}(1-\frac \lambda n)^n (1-\lambda /n)^{-x}
        \]

        We expand to
        \[
            \frac{n(n-1)\cdots()}{}
        \]
    \end{proof}
\end{lemma}

\begin{example}
    Suppose there are 200 people in a class. Assuming that there are no leap years or twins etc., find the probability that two of them were born on January 1st.
    \solution
    Let $X$ be the number of people born on January 1st. The probability that any given person was born on January 1st is $p=\nicefrac 1 {365}$, and we have $n=200$ trials. Thus $X\sim\bin(200, \nicefrac 1 {365})$. However since $n$ is large and $p$ is small, we may approximate $X$ with a Poisson distribution $X\overset{approx}{\sim}\pois(\nicefrac{200}{365})$. This gives us
    \[
        f(2)=\frac{e^{-0.55 } 0.55^2}{2!}\approx 0.087
    \]
    And thus there is approximately a 9\% chance.
\end{example}

\subsubsection{The Poisson Process}
\todo
Let $\{N(t)\}$ be a sequence of a rivals in continuous time. $N(t)$ is a poisson process if 
\begin{enumerate}
    \item $N(0)=0$
    \item The number of arrivals in $[0,t]$ is $Poi(\lambda t)$
    \item Arrivals in disjoint intervals are independent
\end{enumerate}
\begin{example}[Earthquakes]
    The number of earthquakes in an area follow a Poisson process with $\lambda = 6$ (earthquakes per year). Here you're doing lots of trials. You can think of each second as a trial. Find the probability that there are 4 earthquakes in 6 months. Then find the probability that in the next five years, two ofthem will have exactly five earthquakes. 

    \begin{enumerate}
        \item Let $N_t$ be the number of earthquakes in 6 months. Then $N_t\sim \text{Poi}(3)$. Thus we just need to find
        \[
            P(N_t=4)=e^{-3}\frac {3^4}{4!}
        \]
        \item Here we treat each year as a trial, giving us five trials. A success is getting exactly five earthquakes. The number of earthquakes in any one year follows a posson distribution and is
        \[
            p=e^{-5}\frac {6^5}{5!}
        \]
        And we plug this into a Binomial distribution
        \[
            p'=\binom 5 2 p^2(1-p)^{5-2}
        \]
    \end{enumerate}   
\end{example}

\subsection{End-of-the-Section Problems}
\begin{enumerate}
    \item Prove Theorem \ref{thm:propsbin}
    \item Suppose $n$ random strangers meet at a party. Assuming all birthdays are equally likely (and ignoring leap years, twins, etc.). Find the expected number of pairs of people who share a birthday. 
\end{enumerate}


\section{Moment Generating Functions}
Let $X$ be a Random Variable. We define the $n$th \textbf{Moment} of $X$ as $\E(X^n)$. Moments don't really represent anything in the real world on their own, but often appear in formulae for various statistics. In fact, we have already encountered Moments when we talked about Variance.
\[
    \Var(X)=E(X^2)-[E(X)]^2
\]
In other words, Variance can be thought of as the difference between the second Moment and the square of the first Moment. Moments can of course be calculated directly, however it is often easier to use the \textbf{Moment Generating Function} (MGF), which is defined as follows for any Random Variable $X$
\[
    M_X(t)=\E(e^{tX})
\]
\begin{theorem}[Properties of the MGF]
    Let $X$ be a Random Variable (either continuous or discrete)
    \begin{enumerate}
        \item The $n$th Moment of $X$ is given by $M_X^{(n)}(0)$
        \item For any Random Variable $Y$ independent to $X$, $M_Y=M_X$ if and only if $X$ and $Y$ have the same probability distribution
        \item For any Random Variable $Y$ independent to $X$, $M_{X+Y}(t)=M_X(t)M_Y(t)$
    \end{enumerate}
    \begin{proof}
        \begin{enumerate}
            \item[]
            \item Using linearity of Expectation, we can express $M_X(t)$ as a Taylor series
            \[
                M_X(t)=1+t\E(X)+\frac{t^2\E(X^2)}{2!} + \frac{t^3\E(X^3)}{3!} + \cdots 
            \]
            From this we see that the $n$th derivative of $M_X(t)$ is
            \[
                M_X^{(n)}(t)=0+0+\cdots+\E(X^n)+ t\alpha_1(X) + t^2 \alpha_2(X) + \cdots
            \]
            for some $\alpha_1(X), \alpha_2(X),...\in\mathbb R$. Thus
            \[
                M_X^{(n)}(t)=\E(X^n)
            \]
            \item\todo
            \item\todo
        \end{enumerate}
    \end{proof}
\end{theorem}
\label{thm:mgfprops}

\begin{example}
    Find the MGF for the following distributions:
    \begin{enumerate}
        \item Bernoulli
        \item Binomial
    \end{enumerate}
    \solution 
    \begin{enumerate}
        \item Let $I\sim \ber(p)$ for some $p\in[0,1]$ and $q=1-p$. Then
        \[
            M_I(t)=E(e^{tI})=e^0 f(0)+ e^t f(1)= q+pe^t
        \]
        \item Let $X\sim\bin(n,p)$ for some $n\in\{1,2,...\}$ and $p\in[0,1]$. Recall that by the definition of the binomial distribution we may express as the sum of $n$ Independent Bernoulli trials $I_1,I_2,...,I_n\simiid\ber(p)$. From Part 1 we know that
        \[
            \forall i\in\{1,2,...,n\}, M_{I_1}(t)=q+pe^t
        \]
        And thus
        \[
            M_X(t)=M_{I_1+I_2+\cdots+I_n}=\left[q+pe^t\right]^n
        \]
    \end{enumerate}
\end{example}

\begin{example}
        Suppose we find for some Random Variable $Y$
        \[
            M_Y(t)=\tfrac 23 + \tfrac 13 e^t
        \]
        What is the probability distribution of $Y$?
        \solution
        We recognize the MGF of $Y$ as that of the Bernoulli distribution with $p=\nicefrac 13$. Since MGFs are unique, it must be the case that $Y\sim\ber(\nicefrac 13)$
\end{example}



a function which maps Events to real numbers. This definition is very similar to the one we encountered with DRVs, however it presents some new challenges. For instance,

\section{Continuous Random Variables}
Suppose you throw a dart at a dartboard and measure how far from the centre it lands. How might we model this situation mathematically? We will assume that the dart does in fact hit the board, i.e. we assume $P(X\le r)=1$ where $r$ is the radius of the board.




Suppose we throw a dart at a dartboard with some radius $r$ and let $X$ and $Y$ be Random Variables measuring the horizontal and vertical distance respectively of the dart from the centre. Since $X$ and $Y$ represent a distances it is entirely possible for it to take non-integer values, hence they are not DRVs but rather a \textbf{Continuous Random Variables} (CRVs). Now let's assume that $X$ and $Y$ are Independent and that the dart is equally likely hit anywhere on the board. What is the Probability of that the dart hits the exact centre of the board $X=Y=0$? This is one possible point out of an infinite number of possibilities, so the Probability is infinitesimally small. This means that PMFs are not particularly useful in when trying to describe CRVs and so we will not use them. However, it is still perfectly reasonable to ask about the probability of the dart landing on the left half of the board, and so we can still describe 








What is the probability that $X=0$? There are an uncountably infinite number of points on the board the dart could hit, and only one has a distance of exactly zero from the centre. Thus the probability is zero, as is the probability of hitting any exact location. This makes the definition of PMFs useless for CRVs, however we can still use CDFs. For instance, there is a non-zero probability that $X\le 0.5$. And in fact, one arguable benefit of point probabilities being zero is that in all cases for CRVs\[
    P(X\le x)= P(X< x)
\]
Just as in the discrete case, we will often use $F(x)$ to represent the CDF of a CRV, which follows all of the same axioms outlined in \ref{cdf_axioms}, except that it is generally not a step function. And though we cannot use the PMF, we can use the CDF to define its continuous analogue, the \textbf{Probability Density Function} (PDF) $f(x)$ such that
\[
    f(x) = \begin{cases}
        F'(x) & \text{where $F(x)$ is differentiable}
        \\
        0 & \text{otherwise}
    \end{cases}
\]
\begin{theorem}[Properties of the PDF]
    Let $X$ be some CRV with PDF $f(x)$ and CDF $F(x)$. Then
    \begin{enumerate}
        \item $\forall x\in\mathbb R, f(x)\ge 0$
        \item $F(x)=\int_{-\infty}^x f(z)dz$
        \item For any $a,b\in\mathbb R$ such that $b>a$, $P(a\le X\le b)=\int_a^b f(x)$
        \item $\int_{-\infty}^\infty f(x)dx = 1$
    \end{enumerate}
    \begin{proof}
        \begin{enumerate}
            \item[]
            \item Recall from \ref{cdf_axioms} that a CDF is nondecreasing. In other words, its derivative (the PDF) must be nonnegative across its domain.
            \item This follows from the fundamental theorem of calculus and the properties of a CDF, namely the fact that
            \[
                \lim_{x\to-\infty} F(x)=0
            \]
            \item Starting with the LHS we get
            \[
                P(X\le b)=P(X< a)+P(a\le X\le b)
            \]
            giving us
            \[
                P(a\le X\le b)=P(X\le b)-P(X<a)=F(b)-F(a)
            \]
            which is equal to the RHS by the fundamental theorem of calculus.
            \item
            From this we get
            \[
                \int_{-\infty}^\infty f(x)dx = \lim_{x\to \infty} F(x) -\lim_{x\to -\infty} F(x)
            \]
            which by the properties of a CDF is equal to $1$.
        \end{enumerate}
    \end{proof}
\end{theorem}

\begin{notsofast}
    The PDF does not return probabilities. Rather the area underneath the the PDF is a probability. This means that it is completely possible for a PDF to return values greater than 1.
\end{notsofast}

We use the PDF to extend the definition of Expectation to CRVs
\begin{definition}[Expectation of a CRV]
Let $X$ be a CRV with PDF $f(x)$. Then
    \[
        E(X)=\int_{-\infty}^\infty x f(x) dx
    \]
\end{definition}
From this we are able to determine the Variance of a CRV just as we did in the discrete case.

\section{Named Continuous Distributions}
\subsection{The Uniform Distribution}
Once again we start simple. The \textbf{Uniform Distribution} treats all possible values of a CRV $X$ as equally likely, so long as they fall within some finite interval of possible values. This is often a good starting point when we have very little information and must liberally apply Occam's razor. For instance imagine you tie a ribbon to one of the spokes of a bicycle wheel and then let it spin for 3 seconds. Let $X$ be a CRV representing the difference in angle (measured in radians) between the ribbon's starting point and its ending point. If we knew exactly how fast the wheel spun for those 3 seconds, we might be able to predict where exactly the ribbon stopped. But barring that knowledge, it doesn't really make a whole lot of sense to favour any one possibility over another. All we know is that $X$ is between 0 and $2\pi$, and thus it makes sense to treat the PDF of $X$ as a constant on $[0,2\pi)$ (which we will consider equivalent to the closed interval $[0,2\pi]$ since point probabilities are all zero). Symbolically
\[
    f(x)=\begin{cases}
        c & 0\le x \le 2\pi
        \\
        0 & \text{otherwise}
    \end{cases}
\]
for some constant $c$. To find the exact value of $c$ we can take the integral of $f(x)$
\[
    1=\int_{-\infty}^\infty f(x)dx=\int_0^{2\pi} c dx=2\pi c
\]
And thus, dividing both sides by $2\pi$ we get $c=\nicefrac 1 {2\pi}$. More generally, we say that if $X$ has a Uniform Distribution over the interval $[a,b]\subset\mathbb R$ (or equivalently $X\sim U(a,b)$), then the PDF of $X$ is
\[
    f(x)=\begin{cases}
        \frac 1 {b-a} & a\le x \le b
        \\
        0 & \text{otherwise}
    \end{cases}
\]

\begin{theorem}[Properties of the Uniform Distribution]
    Let $X\sim U(a,b)$ for some $a<b$ and let $F(x)$ and $f(x)$ be the CDF and PDF of $X$ respectively. Then
    \begin{enumerate}
        \item $F(x) = \begin{cases}
            0 & x \le a\\
            \displaystyle\frac{x-a}{b-a} & a\le x \le b\\
            1 & x \le b
        \end{cases}$
        \item $M_X(t)=\displaystyle\frac{e^{bt}-e^{at}}{(b-a)t}$
        \item $\E(X)=\displaystyle\frac{a+b} 2$
        \item $\Var(X)=\displaystyle\frac{(b-a)^2}{12}$
    \end{enumerate}
    \begin{proof}
        \begin{enumerate}
            \item[]
            \item We first consider the case that $a\le x\le b$. Then
            \[
                F(x)=\int_a^x \frac 1 {b-a} dz = \frac{x-a}{b-a}
            \]
            In the case that $x<a$, then this is just an integral over zero. And in the case that $x>b$ we can decompose the integral to get
            \[
                F(x)=\int_a^b \frac 1 {b-a} dz + \int_b^x \frac 1 {b-a} dz
            \]
            The first of the two integrals evaluates to 1 and the second is another integral over zero.
            \item Using the definitions of the MGF and the Expectation of a CRV we get
            \[
                M_X(t)=\E\left(e^{tX}\right)=\int_a^b \frac 1 {b-a} e^{tx} dx=\frac{e^{bt}-e^{at}}{(b-a)t}
            \]
            \item Using the definition of Expectation for CRVs we get
            \[
                E(X)=\int_a^b \frac x {b-a} dx=\frac{b^2-a^2}{2(b-a)}
            \]
            Notice that the denominator is a difference of squares. Hence we can factor it and get
            \[
                E(X)=\frac{(b-a)(b+a)}{2(b-a)}=\frac{a+b} 2
            \]
            \item For the second Moment, we get the formula
            \[
                E(X^2)=\int_a^b \frac {x^2} {b-a} dx=\frac{b^3-a^3}{3(b-a)}=\frac{a^2+ab+b^2} 3 = \frac{4a^2+4ab+4b^2}{12}
            \]
            and the square of the first Moment is 
            \[
                [E(X)]^2 = \frac{3a^2+6ab+3b^2}{12}
            \]
            From this we get
            \[
                \Var(X)=\frac{a^2-2ab+b^2} {12}= \frac{(b-a)^2} {12}
            \]
        \end{enumerate}
    \end{proof}
\end{theorem}

\subsubsection{Universality of the Uniform Distribution}
One very useful property of Uniform Distributions is that they are often very easy for computers to simulate, especially $U(0,1)$. It is for this reason that the following theorem is incredibly useful in numerical computation:
\begin{theorem}[The Universality Theorem]
    Let $X$ be some continuous CRV with CDF 
    \[
        F(x)=\begin{cases}
            0 & x\le a
            \\
            \varphi(x) & a\le x \le b
            \\
            1 & x\le b
        \end{cases}
    \]
    for some interval $a\in\mathbb R$, some $b\in\mathbb R\cup\{\infty\}$ and some invertible function $\varphi(x)$ such that $\varphi(a)=0$ and $\lim_{x\to b}\varphi(x)=1$. Then let $Y=F(X)$. Since $X$ (not to be confused with $x$) is not a real number but rather a CRV, $F$ acts as a transformation producing CRV $Y$. The distribution of $Y$ is known. Namely $Y\sim U(0,1)$
    \begin{proof}
        The CDF of $Y$ is 
        \[
            P(Y\le y)= P(F(X)\le y)
        \]
        Since $F$ is a CDF, its range is $[0,1]$. From this we are able to conclude that 
        \begin{align*}
            y\ge 1 &\implies P(F(X)\le y)=1
            \\
            y\le 0 &\implies P(F(X)\le y)=0
        \end{align*}
        We will now consider the case that $0 \le y\le 1$. On this interval, $F(x)=\varphi(x)$ and is therefore invertible, giving us
        \[
            P(Y\le y)=P(X\le\varphi^{-1}(y))=\varphi(\varphi^{-1}(y))=y
        \]
        Thus we get
        \[
            P(Y\le y)=\begin{cases}
                0 & y \le 0
                \\
                y & 0\le y \le 1
                \\
                1 & y \ge 1
            \end{cases}
        \]
        which is simply the CDF for a $U(0,1)$.
    \end{proof}
\end{theorem}

This gives us an easy way to simulate almost all other possible distributions using just the $U(0,1)$. We start by generating some $u$ from a $U(0,1)$ and then transform it to our new distribution using $F^{-1}(u)$.
\begin{example}
    Suppose your boss is interested in a distribution with the following PDF
    \[
        f(x)=\begin{cases}
            e^{-x} & x> 0
            \\
            0 & \text{otherwise}
            \end{cases}
    \]
    You are given $n$ readings from a $U(0,1)$: $u_1, u_2,...,u_n$. Generate $n$ samples following the distribution your boss is interested in.
    \solution
    We first need to calculate the CDF associated with $f(x)$, which we can do by integrating
    \[
        F(x)=\int_{\infty}^x f(z)dz = \begin{cases}
            0 & x < 0
            \\
            1-e^{-x}& x\ge 0
        \end{cases}
    \]
    Notice that for $\phi(x)=1-e^{-x}$
    \begin{enumerate}
        \item $\varphi(0)=0$
        \item $\displaystyle\lim_{x\to\infty}\varphi(x)=1$
        \item $\varphi(x)$ has an inverse, namely $\varphi^{-1}(x)=-\ln(1-y)$
    \end{enumerate}
    Thus $-\ln(1-u_i)$ for $i=1,2,...,n$ follows the distriubtion your boss is interested in.
\end{example}
\todo: THIS FEELS... THERE'S STILL SOME HOLES HERE THAT NEED PATCHING

\subsection{The Exponential Distribution}
\newcommand{\expon}{\textup{Exp}}
Suppose you have a single atom of uranium-235. Being unstable, the atom could spontaneously decay into thorium-231 at any given moment, and you determine that the probability of this happening over the course of any given year is $\lambda$. To model this situation, we could imagine coming into the lab every year on January 1st and checking if the atom has decayed or not. Each year we check is a Bernoulli trial with $p=\lambda$. This means that the number of years $N$ the atom takes to decay would follow a Geometric Distribution and hence the CDF is
\[
    F_1(x)
    = P(N\le x)
    = \begin{cases}
        0 & x \le 0
        \\
        1-(1-\lambda)^x & x\ge 1
    \end{cases}
\]
However, what is the probability that it decays within 2.5 years for example? To tackle this, lets image that we check on the atom every month. These would also be Bernoulli trials, however the probability of a success (i.e. finding thorium instead of uranium) would be twelve times smaller. i.e. 
\[
    P(N_{\frac 1{12}}\le x)
    = \begin{cases}
        0 & x \le 0
        \\
        1-\left(1-\frac \lambda {12}\right)^x & x\ge 1
    \end{cases}
\]
where $N_{\nicefrac 1{12}}$ is the number of months the atom takes to decay. Since $2.5$ years is equivalent to 30 months, the probability that the atom decays within 2.5 years is simply $P(N_{\nicefrac 1{12}} \le 30)$. However, if we still wanted to measure it in years we could create a new CDF for $N$ that supports not only integers, but $\{\nicefrac 1{12} , \nicefrac 2{12}, \nicefrac 3{12},...\}$ as well:
\[
    F_{\frac 1{12}}(x) 
    = P\left(N_{\frac 1{12}}\le 12x\right)
    = \begin{cases}
        0 & x \le 0
        \\
        1-\left(1-\frac \lambda {12}\right)^{12x} & x\ge \frac 1{12}
    \end{cases}
\]
We can generalize this to checking $k$ times per year (at regular intervals) and get
\[
    F_{\frac 1k}(x) 
    = \begin{cases}
        0 & x \le 0
        \\
        1-\left(1-\frac \lambda k\right)^{kx} & x\ge \frac 1k
    \end{cases}
\]
This of course raises the question: what happens as $k\to\infty$, i.e. instead of having distinct trials at regular intervals what if we just watch it 24 hours a day? Doing this we get
\begin{align*}
    \lim_{k\to\infty} F_{\frac 1k}(x) 
    &= 
    \begin{cases}
        0 & x \le 0
        \\
        \displaystyle\lim_{k\to\infty}\left[1-\left(1-\frac \lambda k\right)^{kx}\right] & x\ge \displaystyle\lim_{k\to\infty}\tfrac 1k
    \end{cases}
    \\
    &= 
    \begin{cases}
        0 & x \le 0
        \\
        1-\varphi_\lambda(x) & x > 0
    \end{cases}
\end{align*}
where 
\[
    \varphi_\lambda(x) =\displaystyle\lim_{k\to\infty} \left(1-\frac \lambda k\right)^{kx}
\]
Taking the logarithm of $\varphi_\lambda(x)$ we get
\begin{align*}
    \ln\varphi_\lambda(x) & = \lim_{k\to\infty} kx\ln\left(1-\frac \lambda k\right)
    \\              & = \lim_{k\to\infty} \frac{\ln\left(1-\frac \lambda k\right)}{\frac 1 {kx}}
    \\              & = \lim_{k\to\infty} \frac{\left(\frac 1{1-\nicefrac \lambda k}\right)\left(\frac \lambda {k^2}\right)}{-\frac 1 {k^2 x}} \tag{By L'H${\hat{\textup o}}$pital's Rule}
    \\              & = -\lambda x\lim_{k\to\infty} \left(\frac 1 {1-\frac \lambda k}\right)
    \\              & = -\lambda x
\end{align*}
And hence
\[
    \varphi_\lambda(x)=e^{-\lambda x}
\]
giving us
\[
    \lim_{k\to\infty} F_{\frac 1k}(x) = 
    \begin{cases}
        0 & x \le 0
        \\
        1-e^{-\lambda x} & x > 0
    \end{cases}
\]
Let's call this function $F(x)$ and note that since for all $\lambda \in (0,1)$
\[
    \lim_{x\to 0} \left[1-e^{-\lambda x}\right]=0 
\]
$F(x)$ is continuous and can in fact be written
\[
    F(x) = 
    \begin{cases}
        0 & x \le 0
        \\
        1-e^{-\lambda x} & x \ge 0
    \end{cases}
\]
We will now show that $F(x)$ is a valid continuous CDF
\begin{proof}
    We first show that $F(x)\in[0,1]$ for all $x\in\mathbb R$: Let $x^*\in\mathbb R$. If $x^*\le 0$ then $F(x^*)=0$. If $x^*> 0$ then we know that $e^{-\lambda x^*}\in (0,1)$, and thus $F(x^*)\in (0,1)$. 
    \\\\
    We now show that $F$ is nondecreasing: Let $x_1,x_2\in\mathbb R$ such that $x_1<x_2$. Taking the derivative of $F(x)$ we get
    \[
        \forall x\in(0,\infty), F'(x)=\lambda e^{-\lambda x}
    \]
    which given the properties of the exponential function is positive. Thus if $x_2>x_1>0$ then $F(x_2)>F(x_1)$. In the case that $x_1\le 0$ then we know that $F(x_1)=0$. And since the we have shown that $F$ is nonnegative across its domain, this implies that $F(x_2)\ge F(x_1)$.
    \\\\
    Finally since $F$ is continuous it is also right continuous.     
\end{proof}

It seems we have stumbled upon a new continuous distribution, and in fact this distribution has a name: the \textbf{Exponential Distribution}, often written $X\sim\expon(\lambda)$. The parameter $\lambda$ is known as the ``rate", however there is also another useful quantity $\mu=\nicefrac 1 \lambda$ known as the ``mean". Be advised that some statisticians will use $X\sim\expon(\mu)$ to mean $X$ has an Exponential Distribution with mean $\mu$.
\begin{theorem}[Properties of the Exponential Distribution]
    Let $X\sim\expon(\lambda)$ with PDF $f(x)$ and CDF $F(x)$. Then
    \begin{enumerate}
        \item $f(x)=\begin{cases}
        \lambda e^{-\lambda x} & x \ge 0
        \\
        0 & x < 0
        \end{cases}$
        \item $\E(X)=\displaystyle\frac 1 \lambda$
        \item $\Var(X)=\displaystyle\frac 1 {\lambda^2}$
    \end{enumerate}
    The proofs for all of these properties are very straightforward and are left as an exercise.
\end{theorem}

\begin{theorem}[Memorylessness of the Exponential]
    The Exponential Distribution is the only Memoryless continuous distribution.
    \begin{proof}
        Recall the definition of Memorylessness
        \[
            P(X\ge s+t| X>s)=\frac{1-F(s+t)}{1-F(s)}
            =\frac{e^{-\lambda (s+t)}}{e^{-\lambda s}}
            =e^{-\lambda s}
            =P(X>t)
        \]
        \todo
    \end{proof}
\end{theorem}



\subsection{The Weibull Distribution}
Is the Exponential Distribution good for measuring human lifetime? Suppose we let $X\sim \expon(\lambda)$ represent the lifespan of a person, and imagine we have two people: newborn baby and an 80-year-old. Which of the two do you suppose is most likely to still be alive in 20 years? Certainly the baby, right? However the model we've constructed would say otherwise. Since the Exponential Distribution is Memoryless, our model would tell us that
\[
    P(X\ge 100 | X \ge 80) = P(X\ge 20)
\]
In other words, both people apparently have an equal chance of surviving the next 20 years. This is a flaw in the model, however it is solved by a distribution known as the \textbf{Weibull Distribution} which has the following CDF:
\[
    F(x) = 
    \begin{cases}
        0 & x \le 0
        \\
        1-e^{-[\lambda(x)]x} & x > 0
    \end{cases}
\]
where $\lambda(x)=\lambda_0 x^r$ for some $\lambda_0\in(0,1)$ and $r\in\mathbb R$. Notice that for $r=0$ this is identical to the CDF of the Exponential Distribution with $\lambda =\lambda_0$. However using something like $r=1$ we end up with something resembling an Exponential Distribution, but where the value of $\lambda$ increases linearly with $x$ (i.e. as time passes).  


\subsection{The Gamma and Erlang Distributions}
\newcommand{\gam}{\textup{Gam}}
To talk about the \textbf{Gamma Distribution} we must first talk about the \textbf{Gamma Function} which is defined for all positive real numbers $\alpha$ as
\[
    \Gamma(\alpha)=\int_0^\infty x^{\alpha - 1} e^{-x} dx
\]

\begin{theorem}[Properties of the Gamma Function]
    $ $
    \begin{enumerate}
        \item $\Gamma(\alpha+1)=\alpha\Gamma(\alpha)$
        \item $\forall n\in\{1,2,...\},\Gamma(n+1)=n!$
    \end{enumerate}
    \begin{proof}
        \begin{enumerate}
            \item[]
            \item Plugging $\alpha+1$ into the Gamma function we get
            \begin{align*}
                \Gamma(\alpha+1) & = \int_0^\infty x^{\alpha} e^{-x} dx
                \\             & = -x^\alpha e^{-x}\Big\vert_0^\infty - \int_0^\infty (\alpha x^{\alpha-1}) (-e^{-x}) dx \tag{Using integration by parts}
                \\             & = \alpha \Gamma(\alpha)
            \end{align*}
            \item From this we get that for $n\in\{1,2,...\}$
            \[
                \Gamma(n+1)=n\Gamma(n)=n(n-1)\Gamma(n-1)= \cdots = n! \Gamma(1) = n!
            \]
            since
            \[
                \Gamma(1)=\int_0^\infty e^{-x} dx=1
            \]
        \end{enumerate}
    \end{proof}
\end{theorem}



From this we define the Gamma Distribution
\begin{definition}
    We say $X\sim\gam(\alpha,\beta)$ if the PDF of $X$ is
    \[
        f(x)=\begin{cases}
            \displaystyle\frac {x^{\alpha-1} e^{\frac x \beta}}{\Gamma(\alpha)\beta^\alpha} & x \ge 0
            \\
            0 & x \le 0
        \end{cases}
    \]
\end{definition}
We can show that $f(x)$ as defined above is a valid PDF
\begin{proof}
    Let $t=x/\beta$, we get

\end{proof}

There are some notable special cases of the Gamma Distribution. If $\alpha = 1 $
\[
    f(x)=\frac 1\beta e^{-\frac x \beta}
\]
We should recognize this as the PDF for the Exponential distribution with mean $\beta$. In other words, the Exponential Distribution is simply a special case of the Gamma Distribution. When $\beta=1$ we get
\[
    f(x)=\frac{x^{\alpha-1}e^{-x}}{\Gamma(\alpha)}
\]
This function is interesting since the numerator is simple te integrand of the denominator. In cases where $\alpha$ is an integer, we may call the Gamma Distribtuion the Erlang distribution.
\begin{theorem}
    Let $k$ be any positive integer and let $X\sim \Gamma(\alpha,\beta)$. Then
    \[
        \E(X^k)=\beta^k \frac{\Gamma(\alpha+k)}{F(\alpha}
    \]
    
\end{theorem}

\begin{theorem}[Properties of the Gamma Distribution]
Let $X\sim\gam(\alpha,\beta)$. Then
\begin{enumerate}
    \item $M_X(t)=\frac 1 {(1-\beta t)^\alpha}$
\end{enumerate}
    
\end{theorem}

\begin{theorem}
    {Properties of the Gamma Distribution}
    \begin{enumerate}
        \item[]
        \item The MGF of $X$ is 
        %\item If $\alpha$ is an integer, then $X=Y_1+Y_2+\cdots Y_\alpha$ where $Y_i\simiid \text{Exp(\beta)}$ for $i=1,2,...,\alpha$.
    \end{enumerate}
    \begin{enumerate}
        \item This can be shown using Theorem \todo using $k=1$
        \item We can also find $\E(X^2)$ similarly which gives us
    \end{enumerate}
\end{theorem}

From this we see that the Erlang Distribtuion is the continous analogue of the Negative Binomial.



\subsection{The Normal Distribution}
$X\sim N(\mu,\sigma^2)$ if
\[
    f(x)=\frac 1 {\sqrt{2\pi}\sigma}e^{-\frac 12 (...)}
\]

\[
    \phi(z)=\int_{-\infty}^\infty \frac 1 {2\sqrt{2z}} e^{-\frac {z^2} 2}dz
\]
One very important normal is the standard normal $Z\sim N(0,1)$

Let $t=z^2/2$. Then $dt=zdz$. ...

Why do we are about the normal  distribution.

Pivotal value $Z=\frac{X-\mu}\sigma$

68\% of observations lie within $\mu\pm \sigma$, 95\% of observations lie between 2 standard deviations of the mean. And 99\% of the results lie between 3 standard deviations.

Any time you're testing hypothesis, $3\sigma$ test....


THe main reason why we care about the normal is the central limit theorem. The great thing about the normal distribution is that if you add up a bunch of things, it will beome normal. 

Law of large numbers
Let $X_1,X_2,...,X_n$ be some set of Independent and Identically Distriubted Random Variabes on some distribution such that $E(X_i)=\mu$ and $E$

Eventually everything converges to the average

If we take a lot of measurements, the average will go to their mean when we take a lot of observations

The central limit theorem however says that this will approach with a normal distribution.

All averages start behaving like a normal

Say we have a class project where we go around the city anfd try and find the average income of people in waterloo. If we each take a sample of 50 people, it will approach a normal.

It doesn't matter what $X$ is, it can be discrete continuous, geometric, gamma, 

This seems very general, we make almost no solutions on $X$







\subsection{End-of-the-section Problems}
\begin{enumerate}
    \item Show that the MGF of $X\sim \text{Exp}(\mu)$ \todo
    \item Find the PDF of $X\sim U(0,\nicefrac 1 2)$ and show that it is a valid PDF, but discuss why it could not be used to represent probabilities
\end{enumerate}

\chapter{Multivariate and Conditional Distributions}
Suppose you have a portfolio with three stocks, Google $G$, Tesla $Y$, and $W$ (meta).

We have $50\%$ of our investment in $X$, ... in the newspaper we get these things. However they are not independent. So how do they behave together. This  is called joint distributions. 

How does an individual stock behave? These are called (Marginals). The numbers you see in the newspaper for each stock are the marginals.

How are they corrlated? Often its positively correlated. But what if we look at the corrlation of umbrellas and sun tan lotion. They might be negatively cirrelated. So we might want to diversify by investing in both to achieve more stability, 


We now extend to vector-valued random variables $(X_1,X_2,...,X_m)\sim\mathbb R^n$. 

We are interested in finding how multiple variables are related. Thats a very important hing in stats, especial


\section{Bivariate Discrete Distributions}
Let $X$ and $Y$ be two DRVs which are not independent. Then we can define the joint PMF as
\[
    f(x,y)=P(X=x\land Y=y)
\]
and the CDF as
\[
    F(x,y)=P(X\le x \land Y\le y)
\]

\begin{theorem}
    [Properties of the Discrete Joint PMDF]
    Let $f(x,y) = P(X=...$ be the PDF of som
    \begin{enumerate}
        \item $f(x,y)\ge 0$
        \item $\sum_{x\in\mathbb Z} \sum_{y\in\mathbb Z} f(x,y)$
    \end{enumerate}
\end{theorem}

\begin{example}
    Let $X,Y$ be DRVs with Joint PDF
    \[
        f(x,y)=kq^{x+y-2}p^2
    \]
    Find $k$.
    \solution
    We use the fact that the PMFs have to add up to 1
    \[
        kp^2/q \times \sum q^x \sum q^y
    \]
    Recall geometric series and the fact that 
    \[
        \sum_y q^y = \frac q {1-q}
    \]
\end{example}

\begin{example}
    \[
        f(x,y)=q^{x+y-2}p^2
    \]
    Find the Marginal of $X$
    \solution
    \[
        P(X=x)= \sum_{y\in\mathbb Z} f(x,y)
    \]
\end{example}
You can always get the marginal with a Joint PMF

\begin{example}
    Find $P(X<Y)$
    \solution
    $P(X<$
    You fix one value and look how the other behaves
\end{example}

\section{The Multinomial Distribution}
\newcommand{\mult}{\textup{Mult}}
$n$ is the number of trials, and there are $k$ possible outcomes. Outcome $i$ has a probability $p_i$. The probabilies are constant and the trials are independent. $X_1,X_2,...,X_k$ represents the number of outcomes of type $i$. 

Let's say $k=4$ people are run a 100 meter race every week. Each person has a probability of winning $p_i$ for $i=1,2,3,4$. 




The marginal of a mutlinomail is binomial


\chapter{Stochastic Processes}

There are three stochastic processes we are going to learn, bernoulli, poisson, and markov chains. We alread have a lott of trhe stuff we need

\end{document}
